{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishul/miniconda3/envs/eais/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import DDPMScheduler\n",
    "import random\n",
    "\n",
    "# Training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "diffusion_timesteps = 100\n",
    "dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the diffusion model\n",
    "class OneHotDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.timestep_embed = nn.Embedding(diffusion_timesteps, 16)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 16, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embed(t).squeeze(1)\n",
    "        x = torch.cat([x, t_embed], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# Create synthetic dataset of one-hot vectors\n",
    "class OneHotDataset(Dataset):\n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vec = torch.zeros(dim)\n",
    "        vec[random.randint(0, dim-2)] = 1.0   # exclude the last mode\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sampling utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "def generate_sample(model, scheduler, bsz=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Start with random noise\n",
    "        x = torch.randn((bsz, dim)).to(device)\n",
    "        \n",
    "        # Denoising loop\n",
    "        for t in scheduler.timesteps:\n",
    "            timestep = torch.tensor([t] * bsz, device=device)\n",
    "            noise_pred = model(x, timestep)\n",
    "            x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "        \n",
    "        # Convert to one-hot\n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        sample = torch.argmax(probs, dim=-1)\n",
    "        return F.one_hot(sample, num_classes=dim).squeeze().cpu().numpy()\n",
    "    \n",
    "\n",
    "def generate_and_analyze_samples(model, scheduler, num_samples=10000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate multiple samples at once using batch processing\n",
    "        x = torch.randn((num_samples, dim)).to(device)\n",
    "        \n",
    "        # Denoising loop (same as before but processes all samples simultaneously)\n",
    "        for t in scheduler.timesteps:\n",
    "            timestep = torch.tensor([t], device=device).repeat(num_samples)\n",
    "            noise_pred = model(x, timestep)\n",
    "            x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "        \n",
    "        # Convert to categorical distribution\n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        categories = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        # Convert to one-hot vectors\n",
    "        one_hot_samples = F.one_hot(categories, num_classes=dim)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        counts = torch.bincount(categories, minlength=dim)\n",
    "        stats = {\n",
    "            f\"Category {i}\": count.item() \n",
    "            for i, count in enumerate(counts)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"samples\": one_hot_samples.cpu().numpy(),\n",
    "            \"statistics\": stats,\n",
    "            \"counts\": counts.cpu().numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Train an Unconditional Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishul/miniconda3/envs/eais/lib/python3.10/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 Loss: 0.5866\n",
      "Epoch 2/1000 Loss: 0.4940\n",
      "Epoch 3/1000 Loss: 0.6652\n",
      "Epoch 4/1000 Loss: 0.2852\n",
      "Epoch 5/1000 Loss: 0.2522\n",
      "Epoch 6/1000 Loss: 0.1636\n",
      "Epoch 7/1000 Loss: 0.5530\n",
      "Epoch 8/1000 Loss: 0.1956\n",
      "Epoch 9/1000 Loss: 0.2823\n",
      "Epoch 10/1000 Loss: 0.2502\n",
      "Epoch 11/1000 Loss: 0.2086\n",
      "Epoch 12/1000 Loss: 0.3903\n",
      "Epoch 13/1000 Loss: 0.4961\n",
      "Epoch 14/1000 Loss: 0.4943\n",
      "Epoch 15/1000 Loss: 0.2029\n",
      "Epoch 16/1000 Loss: 0.2318\n",
      "Epoch 17/1000 Loss: 0.2375\n",
      "Epoch 18/1000 Loss: 0.1435\n",
      "Epoch 19/1000 Loss: 0.1689\n",
      "Epoch 20/1000 Loss: 0.3290\n",
      "Epoch 21/1000 Loss: 0.2121\n",
      "Epoch 22/1000 Loss: 0.1811\n",
      "Epoch 23/1000 Loss: 0.1094\n",
      "Epoch 24/1000 Loss: 0.1040\n",
      "Epoch 25/1000 Loss: 0.1011\n",
      "Epoch 26/1000 Loss: 0.0689\n",
      "Epoch 27/1000 Loss: 0.1933\n",
      "Epoch 28/1000 Loss: 0.3659\n",
      "Epoch 29/1000 Loss: 0.1410\n",
      "Epoch 30/1000 Loss: 0.0831\n",
      "Epoch 31/1000 Loss: 0.1133\n",
      "Epoch 32/1000 Loss: 0.0757\n",
      "Epoch 33/1000 Loss: 0.0821\n",
      "Epoch 34/1000 Loss: 0.4685\n",
      "Epoch 35/1000 Loss: 0.4192\n",
      "Epoch 36/1000 Loss: 0.3563\n",
      "Epoch 37/1000 Loss: 0.1017\n",
      "Epoch 38/1000 Loss: 0.0114\n",
      "Epoch 39/1000 Loss: 0.1987\n",
      "Epoch 40/1000 Loss: 0.1770\n",
      "Epoch 41/1000 Loss: 0.0915\n",
      "Epoch 42/1000 Loss: 0.0378\n",
      "Epoch 43/1000 Loss: 0.1002\n",
      "Epoch 44/1000 Loss: 0.0440\n",
      "Epoch 45/1000 Loss: 0.1317\n",
      "Epoch 46/1000 Loss: 0.1258\n",
      "Epoch 47/1000 Loss: 0.1193\n",
      "Epoch 48/1000 Loss: 0.2277\n",
      "Epoch 49/1000 Loss: 0.0418\n",
      "Epoch 50/1000 Loss: 0.0773\n",
      "Epoch 51/1000 Loss: 0.1046\n",
      "Epoch 52/1000 Loss: 0.1458\n",
      "Epoch 53/1000 Loss: 0.0286\n",
      "Epoch 54/1000 Loss: 0.0419\n",
      "Epoch 55/1000 Loss: 0.0819\n",
      "Epoch 56/1000 Loss: 0.1613\n",
      "Epoch 57/1000 Loss: 0.1443\n",
      "Epoch 58/1000 Loss: 0.0423\n",
      "Epoch 59/1000 Loss: 0.0851\n",
      "Epoch 60/1000 Loss: 0.0871\n",
      "Epoch 61/1000 Loss: 0.3013\n",
      "Epoch 62/1000 Loss: 0.1116\n",
      "Epoch 63/1000 Loss: 0.0199\n",
      "Epoch 64/1000 Loss: 0.2070\n",
      "Epoch 65/1000 Loss: 0.0600\n",
      "Epoch 66/1000 Loss: 0.0939\n",
      "Epoch 67/1000 Loss: 0.1229\n",
      "Epoch 68/1000 Loss: 0.1299\n",
      "Epoch 69/1000 Loss: 0.1256\n",
      "Epoch 70/1000 Loss: 0.2205\n",
      "Epoch 71/1000 Loss: 0.0228\n",
      "Epoch 72/1000 Loss: 0.2268\n",
      "Epoch 73/1000 Loss: 0.1823\n",
      "Epoch 74/1000 Loss: 0.0541\n",
      "Epoch 75/1000 Loss: 0.0389\n",
      "Epoch 76/1000 Loss: 0.0477\n",
      "Epoch 77/1000 Loss: 0.0936\n",
      "Epoch 78/1000 Loss: 0.0797\n",
      "Epoch 79/1000 Loss: 0.0871\n",
      "Epoch 80/1000 Loss: 0.1534\n",
      "Epoch 81/1000 Loss: 0.5778\n",
      "Epoch 82/1000 Loss: 0.0930\n",
      "Epoch 83/1000 Loss: 0.0435\n",
      "Epoch 84/1000 Loss: 0.0849\n",
      "Epoch 85/1000 Loss: 0.1028\n",
      "Epoch 86/1000 Loss: 0.0913\n",
      "Epoch 87/1000 Loss: 0.0857\n",
      "Epoch 88/1000 Loss: 0.3568\n",
      "Epoch 89/1000 Loss: 0.1865\n",
      "Epoch 90/1000 Loss: 0.1359\n",
      "Epoch 91/1000 Loss: 0.1043\n",
      "Epoch 92/1000 Loss: 0.0818\n",
      "Epoch 93/1000 Loss: 0.0693\n",
      "Epoch 94/1000 Loss: 0.0621\n",
      "Epoch 95/1000 Loss: 0.0593\n",
      "Epoch 96/1000 Loss: 0.1337\n",
      "Epoch 97/1000 Loss: 0.0229\n",
      "Epoch 98/1000 Loss: 0.1335\n",
      "Epoch 99/1000 Loss: 0.0367\n",
      "Epoch 100/1000 Loss: 0.2648\n",
      "Epoch 101/1000 Loss: 0.0450\n",
      "Epoch 102/1000 Loss: 0.1351\n",
      "Epoch 103/1000 Loss: 0.1082\n",
      "Epoch 104/1000 Loss: 0.0210\n",
      "Epoch 105/1000 Loss: 0.0964\n",
      "Epoch 106/1000 Loss: 0.0745\n",
      "Epoch 107/1000 Loss: 0.1030\n",
      "Epoch 108/1000 Loss: 0.0916\n",
      "Epoch 109/1000 Loss: 0.1657\n",
      "Epoch 110/1000 Loss: 0.0373\n",
      "Epoch 111/1000 Loss: 0.0485\n",
      "Epoch 112/1000 Loss: 0.4937\n",
      "Epoch 113/1000 Loss: 0.0886\n",
      "Epoch 114/1000 Loss: 0.1177\n",
      "Epoch 115/1000 Loss: 0.2429\n",
      "Epoch 116/1000 Loss: 0.0628\n",
      "Epoch 117/1000 Loss: 0.0410\n",
      "Epoch 118/1000 Loss: 0.0653\n",
      "Epoch 119/1000 Loss: 0.1155\n",
      "Epoch 120/1000 Loss: 0.0235\n",
      "Epoch 121/1000 Loss: 0.0260\n",
      "Epoch 122/1000 Loss: 0.0274\n",
      "Epoch 123/1000 Loss: 0.2163\n",
      "Epoch 124/1000 Loss: 0.0860\n",
      "Epoch 125/1000 Loss: 0.0276\n",
      "Epoch 126/1000 Loss: 0.0980\n",
      "Epoch 127/1000 Loss: 0.0488\n",
      "Epoch 128/1000 Loss: 0.0643\n",
      "Epoch 129/1000 Loss: 0.0421\n",
      "Epoch 130/1000 Loss: 0.0748\n",
      "Epoch 131/1000 Loss: 0.0924\n",
      "Epoch 132/1000 Loss: 0.0185\n",
      "Epoch 133/1000 Loss: 0.0807\n",
      "Epoch 134/1000 Loss: 0.0610\n",
      "Epoch 135/1000 Loss: 0.0748\n",
      "Epoch 136/1000 Loss: 0.2037\n",
      "Epoch 137/1000 Loss: 0.0169\n",
      "Epoch 138/1000 Loss: 0.1438\n",
      "Epoch 139/1000 Loss: 0.0756\n",
      "Epoch 140/1000 Loss: 0.1263\n",
      "Epoch 141/1000 Loss: 0.0326\n",
      "Epoch 142/1000 Loss: 0.1210\n",
      "Epoch 143/1000 Loss: 0.0457\n",
      "Epoch 144/1000 Loss: 0.0143\n",
      "Epoch 145/1000 Loss: 0.0331\n",
      "Epoch 146/1000 Loss: 0.1269\n",
      "Epoch 147/1000 Loss: 0.0352\n",
      "Epoch 148/1000 Loss: 0.1210\n",
      "Epoch 149/1000 Loss: 0.4491\n",
      "Epoch 150/1000 Loss: 0.1838\n",
      "Epoch 151/1000 Loss: 0.1543\n",
      "Epoch 152/1000 Loss: 0.0764\n",
      "Epoch 153/1000 Loss: 0.2432\n",
      "Epoch 154/1000 Loss: 0.3131\n",
      "Epoch 155/1000 Loss: 0.2131\n",
      "Epoch 156/1000 Loss: 0.0282\n",
      "Epoch 157/1000 Loss: 0.0338\n",
      "Epoch 158/1000 Loss: 0.0894\n",
      "Epoch 159/1000 Loss: 0.1697\n",
      "Epoch 160/1000 Loss: 0.1336\n",
      "Epoch 161/1000 Loss: 0.0139\n",
      "Epoch 162/1000 Loss: 0.0457\n",
      "Epoch 163/1000 Loss: 0.0906\n",
      "Epoch 164/1000 Loss: 0.0957\n",
      "Epoch 165/1000 Loss: 0.0689\n",
      "Epoch 166/1000 Loss: 0.1520\n",
      "Epoch 167/1000 Loss: 0.0282\n",
      "Epoch 168/1000 Loss: 0.0198\n",
      "Epoch 169/1000 Loss: 0.0377\n",
      "Epoch 170/1000 Loss: 0.1315\n",
      "Epoch 171/1000 Loss: 0.0286\n",
      "Epoch 172/1000 Loss: 0.0480\n",
      "Epoch 173/1000 Loss: 0.0534\n",
      "Epoch 174/1000 Loss: 0.0580\n",
      "Epoch 175/1000 Loss: 0.0520\n",
      "Epoch 176/1000 Loss: 0.0391\n",
      "Epoch 177/1000 Loss: 0.1245\n",
      "Epoch 178/1000 Loss: 0.0922\n",
      "Epoch 179/1000 Loss: 0.0622\n",
      "Epoch 180/1000 Loss: 0.0733\n",
      "Epoch 181/1000 Loss: 0.0995\n",
      "Epoch 182/1000 Loss: 0.1048\n",
      "Epoch 183/1000 Loss: 0.1342\n",
      "Epoch 184/1000 Loss: 0.0211\n",
      "Epoch 185/1000 Loss: 0.0272\n",
      "Epoch 186/1000 Loss: 0.0610\n",
      "Epoch 187/1000 Loss: 0.1759\n",
      "Epoch 188/1000 Loss: 0.0376\n",
      "Epoch 189/1000 Loss: 0.1393\n",
      "Epoch 190/1000 Loss: 0.0335\n",
      "Epoch 191/1000 Loss: 0.1344\n",
      "Epoch 192/1000 Loss: 0.0312\n",
      "Epoch 193/1000 Loss: 0.3304\n",
      "Epoch 194/1000 Loss: 0.0539\n",
      "Epoch 195/1000 Loss: 0.0813\n",
      "Epoch 196/1000 Loss: 0.0881\n",
      "Epoch 197/1000 Loss: 0.0988\n",
      "Epoch 198/1000 Loss: 0.0223\n",
      "Epoch 199/1000 Loss: 0.0938\n",
      "Epoch 200/1000 Loss: 0.0592\n",
      "Epoch 201/1000 Loss: 0.0977\n",
      "Epoch 202/1000 Loss: 0.0790\n",
      "Epoch 203/1000 Loss: 0.0668\n",
      "Epoch 204/1000 Loss: 0.0858\n",
      "Epoch 205/1000 Loss: 0.1431\n",
      "Epoch 206/1000 Loss: 0.0723\n",
      "Epoch 207/1000 Loss: 0.0286\n",
      "Epoch 208/1000 Loss: 0.0335\n",
      "Epoch 209/1000 Loss: 0.0440\n",
      "Epoch 210/1000 Loss: 0.0222\n",
      "Epoch 211/1000 Loss: 0.0335\n",
      "Epoch 212/1000 Loss: 0.1192\n",
      "Epoch 213/1000 Loss: 0.4369\n",
      "Epoch 214/1000 Loss: 0.0390\n",
      "Epoch 215/1000 Loss: 0.0822\n",
      "Epoch 216/1000 Loss: 0.0225\n",
      "Epoch 217/1000 Loss: 0.1356\n",
      "Epoch 218/1000 Loss: 0.1770\n",
      "Epoch 219/1000 Loss: 0.1184\n",
      "Epoch 220/1000 Loss: 0.0243\n",
      "Epoch 221/1000 Loss: 0.0718\n",
      "Epoch 222/1000 Loss: 0.0547\n",
      "Epoch 223/1000 Loss: 0.0602\n",
      "Epoch 224/1000 Loss: 0.0650\n",
      "Epoch 225/1000 Loss: 0.0583\n",
      "Epoch 226/1000 Loss: 0.0842\n",
      "Epoch 227/1000 Loss: 0.1324\n",
      "Epoch 228/1000 Loss: 0.1966\n",
      "Epoch 229/1000 Loss: 0.0965\n",
      "Epoch 230/1000 Loss: 0.0728\n",
      "Epoch 231/1000 Loss: 0.2772\n",
      "Epoch 232/1000 Loss: 0.1854\n",
      "Epoch 233/1000 Loss: 0.0129\n",
      "Epoch 234/1000 Loss: 0.0441\n",
      "Epoch 235/1000 Loss: 0.0415\n",
      "Epoch 236/1000 Loss: 0.0156\n",
      "Epoch 237/1000 Loss: 0.1618\n",
      "Epoch 238/1000 Loss: 0.1002\n",
      "Epoch 239/1000 Loss: 0.0814\n",
      "Epoch 240/1000 Loss: 0.0174\n",
      "Epoch 241/1000 Loss: 0.0959\n",
      "Epoch 242/1000 Loss: 0.1333\n",
      "Epoch 243/1000 Loss: 0.0534\n",
      "Epoch 244/1000 Loss: 0.0603\n",
      "Epoch 245/1000 Loss: 0.2132\n",
      "Epoch 246/1000 Loss: 0.0130\n",
      "Epoch 247/1000 Loss: 0.1384\n",
      "Epoch 248/1000 Loss: 0.0719\n",
      "Epoch 249/1000 Loss: 0.0210\n",
      "Epoch 250/1000 Loss: 0.1239\n",
      "Epoch 251/1000 Loss: 0.0878\n",
      "Epoch 252/1000 Loss: 0.1700\n",
      "Epoch 253/1000 Loss: 0.1859\n",
      "Epoch 254/1000 Loss: 0.0277\n",
      "Epoch 255/1000 Loss: 0.2875\n",
      "Epoch 256/1000 Loss: 0.0807\n",
      "Epoch 257/1000 Loss: 0.1208\n",
      "Epoch 258/1000 Loss: 0.0641\n",
      "Epoch 259/1000 Loss: 0.0658\n",
      "Epoch 260/1000 Loss: 0.1600\n",
      "Epoch 261/1000 Loss: 0.0268\n",
      "Epoch 262/1000 Loss: 0.1071\n",
      "Epoch 263/1000 Loss: 0.1101\n",
      "Epoch 264/1000 Loss: 0.0617\n",
      "Epoch 265/1000 Loss: 0.0580\n",
      "Epoch 266/1000 Loss: 0.0745\n",
      "Epoch 267/1000 Loss: 0.1296\n",
      "Epoch 268/1000 Loss: 0.0374\n",
      "Epoch 269/1000 Loss: 0.0485\n",
      "Epoch 270/1000 Loss: 0.1325\n",
      "Epoch 271/1000 Loss: 0.0316\n",
      "Epoch 272/1000 Loss: 0.0594\n",
      "Epoch 273/1000 Loss: 0.1471\n",
      "Epoch 274/1000 Loss: 0.1959\n",
      "Epoch 275/1000 Loss: 0.1427\n",
      "Epoch 276/1000 Loss: 0.1207\n",
      "Epoch 277/1000 Loss: 0.0285\n",
      "Epoch 278/1000 Loss: 0.0413\n",
      "Epoch 279/1000 Loss: 0.1116\n",
      "Epoch 280/1000 Loss: 0.1960\n",
      "Epoch 281/1000 Loss: 0.1227\n",
      "Epoch 282/1000 Loss: 0.0910\n",
      "Epoch 283/1000 Loss: 0.0558\n",
      "Epoch 284/1000 Loss: 0.1234\n",
      "Epoch 285/1000 Loss: 0.0464\n",
      "Epoch 286/1000 Loss: 0.0334\n",
      "Epoch 287/1000 Loss: 0.0173\n",
      "Epoch 288/1000 Loss: 0.0992\n",
      "Epoch 289/1000 Loss: 0.1175\n",
      "Epoch 290/1000 Loss: 0.0515\n",
      "Epoch 291/1000 Loss: 0.0836\n",
      "Epoch 292/1000 Loss: 0.1524\n",
      "Epoch 293/1000 Loss: 0.0634\n",
      "Epoch 294/1000 Loss: 0.0414\n",
      "Epoch 295/1000 Loss: 0.2524\n",
      "Epoch 296/1000 Loss: 0.0909\n",
      "Epoch 297/1000 Loss: 0.0868\n",
      "Epoch 298/1000 Loss: 0.1353\n",
      "Epoch 299/1000 Loss: 0.0903\n",
      "Epoch 300/1000 Loss: 0.1027\n",
      "Epoch 301/1000 Loss: 0.1084\n",
      "Epoch 302/1000 Loss: 0.0564\n",
      "Epoch 303/1000 Loss: 0.0824\n",
      "Epoch 304/1000 Loss: 0.1013\n",
      "Epoch 305/1000 Loss: 0.0657\n",
      "Epoch 306/1000 Loss: 0.0972\n",
      "Epoch 307/1000 Loss: 0.1776\n",
      "Epoch 308/1000 Loss: 0.0421\n",
      "Epoch 309/1000 Loss: 0.0537\n",
      "Epoch 310/1000 Loss: 0.1333\n",
      "Epoch 311/1000 Loss: 0.1053\n",
      "Epoch 312/1000 Loss: 0.0906\n",
      "Epoch 313/1000 Loss: 0.0389\n",
      "Epoch 314/1000 Loss: 0.0537\n",
      "Epoch 315/1000 Loss: 0.0653\n",
      "Epoch 316/1000 Loss: 0.1891\n",
      "Epoch 317/1000 Loss: 0.0883\n",
      "Epoch 318/1000 Loss: 0.0613\n",
      "Epoch 319/1000 Loss: 0.2703\n",
      "Epoch 320/1000 Loss: 0.0385\n",
      "Epoch 321/1000 Loss: 0.0651\n",
      "Epoch 322/1000 Loss: 0.0494\n",
      "Epoch 323/1000 Loss: 0.0690\n",
      "Epoch 324/1000 Loss: 0.1709\n",
      "Epoch 325/1000 Loss: 0.0127\n",
      "Epoch 326/1000 Loss: 0.0730\n",
      "Epoch 327/1000 Loss: 0.0533\n",
      "Epoch 328/1000 Loss: 0.1701\n",
      "Epoch 329/1000 Loss: 0.0635\n",
      "Epoch 330/1000 Loss: 0.0228\n",
      "Epoch 331/1000 Loss: 0.0762\n",
      "Epoch 332/1000 Loss: 0.0670\n",
      "Epoch 333/1000 Loss: 0.0896\n",
      "Epoch 334/1000 Loss: 0.2313\n",
      "Epoch 335/1000 Loss: 0.1017\n",
      "Epoch 336/1000 Loss: 0.1189\n",
      "Epoch 337/1000 Loss: 0.1049\n",
      "Epoch 338/1000 Loss: 0.0194\n",
      "Epoch 339/1000 Loss: 0.0418\n",
      "Epoch 340/1000 Loss: 0.0914\n",
      "Epoch 341/1000 Loss: 0.0602\n",
      "Epoch 342/1000 Loss: 0.0350\n",
      "Epoch 343/1000 Loss: 0.1127\n",
      "Epoch 344/1000 Loss: 0.1482\n",
      "Epoch 345/1000 Loss: 0.3336\n",
      "Epoch 346/1000 Loss: 0.1172\n",
      "Epoch 347/1000 Loss: 0.0259\n",
      "Epoch 348/1000 Loss: 0.0433\n",
      "Epoch 349/1000 Loss: 0.0267\n",
      "Epoch 350/1000 Loss: 0.0855\n",
      "Epoch 351/1000 Loss: 0.0478\n",
      "Epoch 352/1000 Loss: 0.0854\n",
      "Epoch 353/1000 Loss: 0.2161\n",
      "Epoch 354/1000 Loss: 0.1424\n",
      "Epoch 355/1000 Loss: 0.0745\n",
      "Epoch 356/1000 Loss: 0.1106\n",
      "Epoch 357/1000 Loss: 0.0973\n",
      "Epoch 358/1000 Loss: 0.0838\n",
      "Epoch 359/1000 Loss: 0.0343\n",
      "Epoch 360/1000 Loss: 0.0950\n",
      "Epoch 361/1000 Loss: 0.1262\n",
      "Epoch 362/1000 Loss: 0.1664\n",
      "Epoch 363/1000 Loss: 0.1126\n",
      "Epoch 364/1000 Loss: 0.0642\n",
      "Epoch 365/1000 Loss: 0.0481\n",
      "Epoch 366/1000 Loss: 0.1586\n",
      "Epoch 367/1000 Loss: 0.0454\n",
      "Epoch 368/1000 Loss: 0.0774\n",
      "Epoch 369/1000 Loss: 0.0472\n",
      "Epoch 370/1000 Loss: 0.0787\n",
      "Epoch 371/1000 Loss: 0.1107\n",
      "Epoch 372/1000 Loss: 0.0552\n",
      "Epoch 373/1000 Loss: 0.0542\n",
      "Epoch 374/1000 Loss: 0.0626\n",
      "Epoch 375/1000 Loss: 0.0144\n",
      "Epoch 376/1000 Loss: 0.1787\n",
      "Epoch 377/1000 Loss: 0.0330\n",
      "Epoch 378/1000 Loss: 0.0553\n",
      "Epoch 379/1000 Loss: 0.1290\n",
      "Epoch 380/1000 Loss: 0.2201\n",
      "Epoch 381/1000 Loss: 0.0393\n",
      "Epoch 382/1000 Loss: 0.0467\n",
      "Epoch 383/1000 Loss: 0.0215\n",
      "Epoch 384/1000 Loss: 0.1316\n",
      "Epoch 385/1000 Loss: 0.0692\n",
      "Epoch 386/1000 Loss: 0.0894\n",
      "Epoch 387/1000 Loss: 0.0250\n",
      "Epoch 388/1000 Loss: 0.0585\n",
      "Epoch 389/1000 Loss: 0.1570\n",
      "Epoch 390/1000 Loss: 0.1258\n",
      "Epoch 391/1000 Loss: 0.0598\n",
      "Epoch 392/1000 Loss: 0.0337\n",
      "Epoch 393/1000 Loss: 0.0223\n",
      "Epoch 394/1000 Loss: 0.1012\n",
      "Epoch 395/1000 Loss: 0.0834\n",
      "Epoch 396/1000 Loss: 0.1307\n",
      "Epoch 397/1000 Loss: 0.0237\n",
      "Epoch 398/1000 Loss: 0.2194\n",
      "Epoch 399/1000 Loss: 0.0991\n",
      "Epoch 400/1000 Loss: 0.0368\n",
      "Epoch 401/1000 Loss: 0.0975\n",
      "Epoch 402/1000 Loss: 0.1488\n",
      "Epoch 403/1000 Loss: 0.1375\n",
      "Epoch 404/1000 Loss: 0.1833\n",
      "Epoch 405/1000 Loss: 0.0712\n",
      "Epoch 406/1000 Loss: 0.0456\n",
      "Epoch 407/1000 Loss: 0.1097\n",
      "Epoch 408/1000 Loss: 0.0554\n",
      "Epoch 409/1000 Loss: 0.2019\n",
      "Epoch 410/1000 Loss: 0.0688\n",
      "Epoch 411/1000 Loss: 0.0314\n",
      "Epoch 412/1000 Loss: 0.0810\n",
      "Epoch 413/1000 Loss: 0.1710\n",
      "Epoch 414/1000 Loss: 0.0485\n",
      "Epoch 415/1000 Loss: 0.0717\n",
      "Epoch 416/1000 Loss: 0.0785\n",
      "Epoch 417/1000 Loss: 0.1362\n",
      "Epoch 418/1000 Loss: 0.0286\n",
      "Epoch 419/1000 Loss: 0.0582\n",
      "Epoch 420/1000 Loss: 0.0678\n",
      "Epoch 421/1000 Loss: 0.0264\n",
      "Epoch 422/1000 Loss: 0.0513\n",
      "Epoch 423/1000 Loss: 0.1375\n",
      "Epoch 424/1000 Loss: 0.0633\n",
      "Epoch 425/1000 Loss: 0.1457\n",
      "Epoch 426/1000 Loss: 0.1625\n",
      "Epoch 427/1000 Loss: 0.0856\n",
      "Epoch 428/1000 Loss: 0.0419\n",
      "Epoch 429/1000 Loss: 0.1018\n",
      "Epoch 430/1000 Loss: 0.0467\n",
      "Epoch 431/1000 Loss: 0.0721\n",
      "Epoch 432/1000 Loss: 0.0524\n",
      "Epoch 433/1000 Loss: 0.0410\n",
      "Epoch 434/1000 Loss: 0.0459\n",
      "Epoch 435/1000 Loss: 0.0719\n",
      "Epoch 436/1000 Loss: 0.0557\n",
      "Epoch 437/1000 Loss: 0.0306\n",
      "Epoch 438/1000 Loss: 0.0206\n",
      "Epoch 439/1000 Loss: 0.1053\n",
      "Epoch 440/1000 Loss: 0.1068\n",
      "Epoch 441/1000 Loss: 0.0400\n",
      "Epoch 442/1000 Loss: 0.0745\n",
      "Epoch 443/1000 Loss: 0.0410\n",
      "Epoch 444/1000 Loss: 0.0767\n",
      "Epoch 445/1000 Loss: 0.0824\n",
      "Epoch 446/1000 Loss: 0.0881\n",
      "Epoch 447/1000 Loss: 0.0482\n",
      "Epoch 448/1000 Loss: 0.2569\n",
      "Epoch 449/1000 Loss: 0.5114\n",
      "Epoch 450/1000 Loss: 0.0820\n",
      "Epoch 451/1000 Loss: 0.0685\n",
      "Epoch 452/1000 Loss: 0.1115\n",
      "Epoch 453/1000 Loss: 0.0738\n",
      "Epoch 454/1000 Loss: 0.0600\n",
      "Epoch 455/1000 Loss: 0.0958\n",
      "Epoch 456/1000 Loss: 0.0314\n",
      "Epoch 457/1000 Loss: 0.1008\n",
      "Epoch 458/1000 Loss: 0.1173\n",
      "Epoch 459/1000 Loss: 0.1211\n",
      "Epoch 460/1000 Loss: 0.0371\n",
      "Epoch 461/1000 Loss: 0.0427\n",
      "Epoch 462/1000 Loss: 0.1237\n",
      "Epoch 463/1000 Loss: 0.0310\n",
      "Epoch 464/1000 Loss: 0.0410\n",
      "Epoch 465/1000 Loss: 0.1329\n",
      "Epoch 466/1000 Loss: 0.0223\n",
      "Epoch 467/1000 Loss: 0.0598\n",
      "Epoch 468/1000 Loss: 0.0783\n",
      "Epoch 469/1000 Loss: 0.0587\n",
      "Epoch 470/1000 Loss: 0.0855\n",
      "Epoch 471/1000 Loss: 0.0688\n",
      "Epoch 472/1000 Loss: 0.0813\n",
      "Epoch 473/1000 Loss: 0.0966\n",
      "Epoch 474/1000 Loss: 0.1988\n",
      "Epoch 475/1000 Loss: 0.0520\n",
      "Epoch 476/1000 Loss: 0.3017\n",
      "Epoch 477/1000 Loss: 0.0877\n",
      "Epoch 478/1000 Loss: 0.1375\n",
      "Epoch 479/1000 Loss: 0.0986\n",
      "Epoch 480/1000 Loss: 0.2571\n",
      "Epoch 481/1000 Loss: 0.0266\n",
      "Epoch 482/1000 Loss: 0.0946\n",
      "Epoch 483/1000 Loss: 0.0665\n",
      "Epoch 484/1000 Loss: 0.0227\n",
      "Epoch 485/1000 Loss: 0.0706\n",
      "Epoch 486/1000 Loss: 0.3287\n",
      "Epoch 487/1000 Loss: 0.1466\n",
      "Epoch 488/1000 Loss: 0.1068\n",
      "Epoch 489/1000 Loss: 0.0514\n",
      "Epoch 490/1000 Loss: 0.0231\n",
      "Epoch 491/1000 Loss: 0.0475\n",
      "Epoch 492/1000 Loss: 0.1810\n",
      "Epoch 493/1000 Loss: 0.0171\n",
      "Epoch 494/1000 Loss: 0.0374\n",
      "Epoch 495/1000 Loss: 0.1513\n",
      "Epoch 496/1000 Loss: 0.3972\n",
      "Epoch 497/1000 Loss: 0.0304\n",
      "Epoch 498/1000 Loss: 0.0094\n",
      "Epoch 499/1000 Loss: 0.1507\n",
      "Epoch 500/1000 Loss: 0.1139\n",
      "Epoch 501/1000 Loss: 0.1303\n",
      "Epoch 502/1000 Loss: 0.0769\n",
      "Epoch 503/1000 Loss: 0.1196\n",
      "Epoch 504/1000 Loss: 0.0893\n",
      "Epoch 505/1000 Loss: 0.2869\n",
      "Epoch 506/1000 Loss: 0.0979\n",
      "Epoch 507/1000 Loss: 0.0398\n",
      "Epoch 508/1000 Loss: 0.0580\n",
      "Epoch 509/1000 Loss: 0.0845\n",
      "Epoch 510/1000 Loss: 0.0859\n",
      "Epoch 511/1000 Loss: 0.0579\n",
      "Epoch 512/1000 Loss: 0.0891\n",
      "Epoch 513/1000 Loss: 0.1766\n",
      "Epoch 514/1000 Loss: 0.0358\n",
      "Epoch 515/1000 Loss: 0.1140\n",
      "Epoch 516/1000 Loss: 0.1128\n",
      "Epoch 517/1000 Loss: 0.1851\n",
      "Epoch 518/1000 Loss: 0.0643\n",
      "Epoch 519/1000 Loss: 0.0375\n",
      "Epoch 520/1000 Loss: 0.0345\n",
      "Epoch 521/1000 Loss: 0.3818\n",
      "Epoch 522/1000 Loss: 0.0213\n",
      "Epoch 523/1000 Loss: 0.0759\n",
      "Epoch 524/1000 Loss: 0.0321\n",
      "Epoch 525/1000 Loss: 0.2082\n",
      "Epoch 526/1000 Loss: 0.0757\n",
      "Epoch 527/1000 Loss: 0.0158\n",
      "Epoch 528/1000 Loss: 0.0369\n",
      "Epoch 529/1000 Loss: 0.2112\n",
      "Epoch 530/1000 Loss: 0.0709\n",
      "Epoch 531/1000 Loss: 0.0218\n",
      "Epoch 532/1000 Loss: 0.1258\n",
      "Epoch 533/1000 Loss: 0.0680\n",
      "Epoch 534/1000 Loss: 0.1138\n",
      "Epoch 535/1000 Loss: 0.0564\n",
      "Epoch 536/1000 Loss: 0.0835\n",
      "Epoch 537/1000 Loss: 0.0158\n",
      "Epoch 538/1000 Loss: 0.0576\n",
      "Epoch 539/1000 Loss: 0.0593\n",
      "Epoch 540/1000 Loss: 0.1291\n",
      "Epoch 541/1000 Loss: 0.0220\n",
      "Epoch 542/1000 Loss: 0.0398\n",
      "Epoch 543/1000 Loss: 0.0290\n",
      "Epoch 544/1000 Loss: 0.1837\n",
      "Epoch 545/1000 Loss: 0.0488\n",
      "Epoch 546/1000 Loss: 0.0831\n",
      "Epoch 547/1000 Loss: 0.0796\n",
      "Epoch 548/1000 Loss: 0.0659\n",
      "Epoch 549/1000 Loss: 0.2999\n",
      "Epoch 550/1000 Loss: 0.0919\n",
      "Epoch 551/1000 Loss: 0.0146\n",
      "Epoch 552/1000 Loss: 0.0570\n",
      "Epoch 553/1000 Loss: 0.0253\n",
      "Epoch 554/1000 Loss: 0.1206\n",
      "Epoch 555/1000 Loss: 0.0305\n",
      "Epoch 556/1000 Loss: 0.0934\n",
      "Epoch 557/1000 Loss: 0.1955\n",
      "Epoch 558/1000 Loss: 0.2163\n",
      "Epoch 559/1000 Loss: 0.0541\n",
      "Epoch 560/1000 Loss: 0.0122\n",
      "Epoch 561/1000 Loss: 0.0539\n",
      "Epoch 562/1000 Loss: 0.1121\n",
      "Epoch 563/1000 Loss: 0.2005\n",
      "Epoch 564/1000 Loss: 0.0315\n",
      "Epoch 565/1000 Loss: 0.0160\n",
      "Epoch 566/1000 Loss: 0.1102\n",
      "Epoch 567/1000 Loss: 0.0130\n",
      "Epoch 568/1000 Loss: 0.0807\n",
      "Epoch 569/1000 Loss: 0.0721\n",
      "Epoch 570/1000 Loss: 0.2046\n",
      "Epoch 571/1000 Loss: 0.0566\n",
      "Epoch 572/1000 Loss: 0.0197\n",
      "Epoch 573/1000 Loss: 0.0725\n",
      "Epoch 574/1000 Loss: 0.0839\n",
      "Epoch 575/1000 Loss: 0.0335\n",
      "Epoch 576/1000 Loss: 0.1021\n",
      "Epoch 577/1000 Loss: 0.0345\n",
      "Epoch 578/1000 Loss: 0.2070\n",
      "Epoch 579/1000 Loss: 0.2305\n",
      "Epoch 580/1000 Loss: 0.1134\n",
      "Epoch 581/1000 Loss: 0.0706\n",
      "Epoch 582/1000 Loss: 0.1059\n",
      "Epoch 583/1000 Loss: 0.0141\n",
      "Epoch 584/1000 Loss: 0.0749\n",
      "Epoch 585/1000 Loss: 0.0531\n",
      "Epoch 586/1000 Loss: 0.0421\n",
      "Epoch 587/1000 Loss: 0.0809\n",
      "Epoch 588/1000 Loss: 0.1015\n",
      "Epoch 589/1000 Loss: 0.0583\n",
      "Epoch 590/1000 Loss: 0.0256\n",
      "Epoch 591/1000 Loss: 0.1222\n",
      "Epoch 592/1000 Loss: 0.0546\n",
      "Epoch 593/1000 Loss: 0.0206\n",
      "Epoch 594/1000 Loss: 0.3123\n",
      "Epoch 595/1000 Loss: 0.0997\n",
      "Epoch 596/1000 Loss: 0.0721\n",
      "Epoch 597/1000 Loss: 0.1719\n",
      "Epoch 598/1000 Loss: 0.0992\n",
      "Epoch 599/1000 Loss: 0.2937\n",
      "Epoch 600/1000 Loss: 0.0671\n",
      "Epoch 601/1000 Loss: 0.0887\n",
      "Epoch 602/1000 Loss: 0.1109\n",
      "Epoch 603/1000 Loss: 0.0969\n",
      "Epoch 604/1000 Loss: 0.1393\n",
      "Epoch 605/1000 Loss: 0.1789\n",
      "Epoch 606/1000 Loss: 0.0694\n",
      "Epoch 607/1000 Loss: 0.0143\n",
      "Epoch 608/1000 Loss: 0.1138\n",
      "Epoch 609/1000 Loss: 0.0610\n",
      "Epoch 610/1000 Loss: 0.0723\n",
      "Epoch 611/1000 Loss: 0.0434\n",
      "Epoch 612/1000 Loss: 0.0200\n",
      "Epoch 613/1000 Loss: 0.0294\n",
      "Epoch 614/1000 Loss: 0.0412\n",
      "Epoch 615/1000 Loss: 0.1198\n",
      "Epoch 616/1000 Loss: 0.0589\n",
      "Epoch 617/1000 Loss: 0.0250\n",
      "Epoch 618/1000 Loss: 0.1416\n",
      "Epoch 619/1000 Loss: 0.0634\n",
      "Epoch 620/1000 Loss: 0.1567\n",
      "Epoch 621/1000 Loss: 0.0568\n",
      "Epoch 622/1000 Loss: 0.0539\n",
      "Epoch 623/1000 Loss: 0.1404\n",
      "Epoch 624/1000 Loss: 0.2165\n",
      "Epoch 625/1000 Loss: 0.0872\n",
      "Epoch 626/1000 Loss: 0.0436\n",
      "Epoch 627/1000 Loss: 0.0802\n",
      "Epoch 628/1000 Loss: 0.0183\n",
      "Epoch 629/1000 Loss: 0.0464\n",
      "Epoch 630/1000 Loss: 0.1000\n",
      "Epoch 631/1000 Loss: 0.1653\n",
      "Epoch 632/1000 Loss: 0.1123\n",
      "Epoch 633/1000 Loss: 0.1119\n",
      "Epoch 634/1000 Loss: 0.0235\n",
      "Epoch 635/1000 Loss: 0.0321\n",
      "Epoch 636/1000 Loss: 0.0620\n",
      "Epoch 637/1000 Loss: 0.0679\n",
      "Epoch 638/1000 Loss: 0.0585\n",
      "Epoch 639/1000 Loss: 0.1270\n",
      "Epoch 640/1000 Loss: 0.0505\n",
      "Epoch 641/1000 Loss: 0.3572\n",
      "Epoch 642/1000 Loss: 0.0394\n",
      "Epoch 643/1000 Loss: 0.0719\n",
      "Epoch 644/1000 Loss: 0.0290\n",
      "Epoch 645/1000 Loss: 0.1198\n",
      "Epoch 646/1000 Loss: 0.0458\n",
      "Epoch 647/1000 Loss: 0.0955\n",
      "Epoch 648/1000 Loss: 0.0695\n",
      "Epoch 649/1000 Loss: 0.0098\n",
      "Epoch 650/1000 Loss: 0.0505\n",
      "Epoch 651/1000 Loss: 0.0811\n",
      "Epoch 652/1000 Loss: 0.1648\n",
      "Epoch 653/1000 Loss: 0.0366\n",
      "Epoch 654/1000 Loss: 0.0509\n",
      "Epoch 655/1000 Loss: 0.0455\n",
      "Epoch 656/1000 Loss: 0.2728\n",
      "Epoch 657/1000 Loss: 0.0057\n",
      "Epoch 658/1000 Loss: 0.1309\n",
      "Epoch 659/1000 Loss: 0.1052\n",
      "Epoch 660/1000 Loss: 0.0839\n",
      "Epoch 661/1000 Loss: 0.0679\n",
      "Epoch 662/1000 Loss: 0.1575\n",
      "Epoch 663/1000 Loss: 0.1782\n",
      "Epoch 664/1000 Loss: 0.0303\n",
      "Epoch 665/1000 Loss: 0.0339\n",
      "Epoch 666/1000 Loss: 0.0278\n",
      "Epoch 667/1000 Loss: 0.0430\n",
      "Epoch 668/1000 Loss: 0.0465\n",
      "Epoch 669/1000 Loss: 0.0192\n",
      "Epoch 670/1000 Loss: 0.0358\n",
      "Epoch 671/1000 Loss: 0.1333\n",
      "Epoch 672/1000 Loss: 0.0709\n",
      "Epoch 673/1000 Loss: 0.0475\n",
      "Epoch 674/1000 Loss: 0.1312\n",
      "Epoch 675/1000 Loss: 0.0594\n",
      "Epoch 676/1000 Loss: 0.0673\n",
      "Epoch 677/1000 Loss: 0.0259\n",
      "Epoch 678/1000 Loss: 0.0611\n",
      "Epoch 679/1000 Loss: 0.0796\n",
      "Epoch 680/1000 Loss: 0.0190\n",
      "Epoch 681/1000 Loss: 0.0516\n",
      "Epoch 682/1000 Loss: 0.0375\n",
      "Epoch 683/1000 Loss: 0.1143\n",
      "Epoch 684/1000 Loss: 0.1480\n",
      "Epoch 685/1000 Loss: 0.0779\n",
      "Epoch 686/1000 Loss: 0.0840\n",
      "Epoch 687/1000 Loss: 0.3523\n",
      "Epoch 688/1000 Loss: 0.3197\n",
      "Epoch 689/1000 Loss: 0.0378\n",
      "Epoch 690/1000 Loss: 0.5653\n",
      "Epoch 691/1000 Loss: 0.0567\n",
      "Epoch 692/1000 Loss: 0.0629\n",
      "Epoch 693/1000 Loss: 0.0645\n",
      "Epoch 694/1000 Loss: 0.0700\n",
      "Epoch 695/1000 Loss: 0.1319\n",
      "Epoch 696/1000 Loss: 0.3135\n",
      "Epoch 697/1000 Loss: 0.0349\n",
      "Epoch 698/1000 Loss: 0.0982\n",
      "Epoch 699/1000 Loss: 0.1618\n",
      "Epoch 700/1000 Loss: 0.1817\n",
      "Epoch 701/1000 Loss: 0.0171\n",
      "Epoch 702/1000 Loss: 0.0739\n",
      "Epoch 703/1000 Loss: 0.1054\n",
      "Epoch 704/1000 Loss: 0.1589\n",
      "Epoch 705/1000 Loss: 0.0543\n",
      "Epoch 706/1000 Loss: 0.0311\n",
      "Epoch 707/1000 Loss: 0.0757\n",
      "Epoch 708/1000 Loss: 0.1604\n",
      "Epoch 709/1000 Loss: 0.0287\n",
      "Epoch 710/1000 Loss: 0.0306\n",
      "Epoch 711/1000 Loss: 0.0286\n",
      "Epoch 712/1000 Loss: 0.0511\n",
      "Epoch 713/1000 Loss: 0.0141\n",
      "Epoch 714/1000 Loss: 0.1722\n",
      "Epoch 715/1000 Loss: 0.1682\n",
      "Epoch 716/1000 Loss: 0.1133\n",
      "Epoch 717/1000 Loss: 0.3538\n",
      "Epoch 718/1000 Loss: 0.1048\n",
      "Epoch 719/1000 Loss: 0.0308\n",
      "Epoch 720/1000 Loss: 0.0829\n",
      "Epoch 721/1000 Loss: 0.0362\n",
      "Epoch 722/1000 Loss: 0.1038\n",
      "Epoch 723/1000 Loss: 0.0216\n",
      "Epoch 724/1000 Loss: 0.0231\n",
      "Epoch 725/1000 Loss: 0.0404\n",
      "Epoch 726/1000 Loss: 0.0589\n",
      "Epoch 727/1000 Loss: 0.0640\n",
      "Epoch 728/1000 Loss: 0.0297\n",
      "Epoch 729/1000 Loss: 0.0493\n",
      "Epoch 730/1000 Loss: 0.0827\n",
      "Epoch 731/1000 Loss: 0.1607\n",
      "Epoch 732/1000 Loss: 0.0670\n",
      "Epoch 733/1000 Loss: 0.0394\n",
      "Epoch 734/1000 Loss: 0.0710\n",
      "Epoch 735/1000 Loss: 0.1295\n",
      "Epoch 736/1000 Loss: 0.1637\n",
      "Epoch 737/1000 Loss: 0.2333\n",
      "Epoch 738/1000 Loss: 0.0875\n",
      "Epoch 739/1000 Loss: 0.2724\n",
      "Epoch 740/1000 Loss: 0.0849\n",
      "Epoch 741/1000 Loss: 0.0357\n",
      "Epoch 742/1000 Loss: 0.0101\n",
      "Epoch 743/1000 Loss: 0.1687\n",
      "Epoch 744/1000 Loss: 0.1053\n",
      "Epoch 745/1000 Loss: 0.2290\n",
      "Epoch 746/1000 Loss: 0.1313\n",
      "Epoch 747/1000 Loss: 0.0328\n",
      "Epoch 748/1000 Loss: 0.0293\n",
      "Epoch 749/1000 Loss: 0.0781\n",
      "Epoch 750/1000 Loss: 0.0974\n",
      "Epoch 751/1000 Loss: 0.0342\n",
      "Epoch 752/1000 Loss: 0.1604\n",
      "Epoch 753/1000 Loss: 0.1490\n",
      "Epoch 754/1000 Loss: 0.1509\n",
      "Epoch 755/1000 Loss: 0.0793\n",
      "Epoch 756/1000 Loss: 0.0177\n",
      "Epoch 757/1000 Loss: 0.0459\n",
      "Epoch 758/1000 Loss: 0.0525\n",
      "Epoch 759/1000 Loss: 0.0489\n",
      "Epoch 760/1000 Loss: 0.0855\n",
      "Epoch 761/1000 Loss: 0.1459\n",
      "Epoch 762/1000 Loss: 0.0660\n",
      "Epoch 763/1000 Loss: 0.1610\n",
      "Epoch 764/1000 Loss: 0.1030\n",
      "Epoch 765/1000 Loss: 0.5913\n",
      "Epoch 766/1000 Loss: 0.1171\n",
      "Epoch 767/1000 Loss: 0.1338\n",
      "Epoch 768/1000 Loss: 0.0728\n",
      "Epoch 769/1000 Loss: 0.1962\n",
      "Epoch 770/1000 Loss: 0.0391\n",
      "Epoch 771/1000 Loss: 0.0274\n",
      "Epoch 772/1000 Loss: 0.1550\n",
      "Epoch 773/1000 Loss: 0.1275\n",
      "Epoch 774/1000 Loss: 0.0919\n",
      "Epoch 775/1000 Loss: 0.0692\n",
      "Epoch 776/1000 Loss: 0.0250\n",
      "Epoch 777/1000 Loss: 0.0128\n",
      "Epoch 778/1000 Loss: 0.0714\n",
      "Epoch 779/1000 Loss: 0.1322\n",
      "Epoch 780/1000 Loss: 0.0371\n",
      "Epoch 781/1000 Loss: 0.0897\n",
      "Epoch 782/1000 Loss: 0.0406\n",
      "Epoch 783/1000 Loss: 0.0845\n",
      "Epoch 784/1000 Loss: 0.0304\n",
      "Epoch 785/1000 Loss: 0.1343\n",
      "Epoch 786/1000 Loss: 0.0520\n",
      "Epoch 787/1000 Loss: 0.1102\n",
      "Epoch 788/1000 Loss: 0.0560\n",
      "Epoch 789/1000 Loss: 0.4557\n",
      "Epoch 790/1000 Loss: 0.1733\n",
      "Epoch 791/1000 Loss: 0.0183\n",
      "Epoch 792/1000 Loss: 0.1416\n",
      "Epoch 793/1000 Loss: 0.0677\n",
      "Epoch 794/1000 Loss: 0.0806\n",
      "Epoch 795/1000 Loss: 0.0564\n",
      "Epoch 796/1000 Loss: 0.0408\n",
      "Epoch 797/1000 Loss: 0.1156\n",
      "Epoch 798/1000 Loss: 0.0251\n",
      "Epoch 799/1000 Loss: 0.0636\n",
      "Epoch 800/1000 Loss: 0.1085\n",
      "Epoch 801/1000 Loss: 0.1611\n",
      "Epoch 802/1000 Loss: 0.1350\n",
      "Epoch 803/1000 Loss: 0.1297\n",
      "Epoch 804/1000 Loss: 0.0265\n",
      "Epoch 805/1000 Loss: 0.0949\n",
      "Epoch 806/1000 Loss: 0.0622\n",
      "Epoch 807/1000 Loss: 0.0916\n",
      "Epoch 808/1000 Loss: 0.0398\n",
      "Epoch 809/1000 Loss: 0.0738\n",
      "Epoch 810/1000 Loss: 0.0806\n",
      "Epoch 811/1000 Loss: 0.0673\n",
      "Epoch 812/1000 Loss: 0.0961\n",
      "Epoch 813/1000 Loss: 0.0759\n",
      "Epoch 814/1000 Loss: 0.0870\n",
      "Epoch 815/1000 Loss: 0.0564\n",
      "Epoch 816/1000 Loss: 0.0357\n",
      "Epoch 817/1000 Loss: 0.1453\n",
      "Epoch 818/1000 Loss: 0.0811\n",
      "Epoch 819/1000 Loss: 0.0716\n",
      "Epoch 820/1000 Loss: 0.0082\n",
      "Epoch 821/1000 Loss: 0.0757\n",
      "Epoch 822/1000 Loss: 0.0762\n",
      "Epoch 823/1000 Loss: 0.0505\n",
      "Epoch 824/1000 Loss: 0.1158\n",
      "Epoch 825/1000 Loss: 0.0522\n",
      "Epoch 826/1000 Loss: 0.1580\n",
      "Epoch 827/1000 Loss: 0.1719\n",
      "Epoch 828/1000 Loss: 0.2626\n",
      "Epoch 829/1000 Loss: 0.0732\n",
      "Epoch 830/1000 Loss: 0.0396\n",
      "Epoch 831/1000 Loss: 0.0242\n",
      "Epoch 832/1000 Loss: 0.3477\n",
      "Epoch 833/1000 Loss: 0.0496\n",
      "Epoch 834/1000 Loss: 0.1377\n",
      "Epoch 835/1000 Loss: 0.0264\n",
      "Epoch 836/1000 Loss: 0.0208\n",
      "Epoch 837/1000 Loss: 0.0403\n",
      "Epoch 838/1000 Loss: 0.1659\n",
      "Epoch 839/1000 Loss: 0.0914\n",
      "Epoch 840/1000 Loss: 0.0708\n",
      "Epoch 841/1000 Loss: 0.0288\n",
      "Epoch 842/1000 Loss: 0.3133\n",
      "Epoch 843/1000 Loss: 0.1682\n",
      "Epoch 844/1000 Loss: 0.1268\n",
      "Epoch 845/1000 Loss: 0.1160\n",
      "Epoch 846/1000 Loss: 0.1929\n",
      "Epoch 847/1000 Loss: 0.2002\n",
      "Epoch 848/1000 Loss: 0.0376\n",
      "Epoch 849/1000 Loss: 0.1103\n",
      "Epoch 850/1000 Loss: 0.0907\n",
      "Epoch 851/1000 Loss: 0.1353\n",
      "Epoch 852/1000 Loss: 0.0511\n",
      "Epoch 853/1000 Loss: 0.0377\n",
      "Epoch 854/1000 Loss: 0.1612\n",
      "Epoch 855/1000 Loss: 0.0301\n",
      "Epoch 856/1000 Loss: 0.1478\n",
      "Epoch 857/1000 Loss: 0.2416\n",
      "Epoch 858/1000 Loss: 0.0905\n",
      "Epoch 859/1000 Loss: 0.1926\n",
      "Epoch 860/1000 Loss: 0.0946\n",
      "Epoch 861/1000 Loss: 0.2034\n",
      "Epoch 862/1000 Loss: 0.0499\n",
      "Epoch 863/1000 Loss: 0.0923\n",
      "Epoch 864/1000 Loss: 0.0519\n",
      "Epoch 865/1000 Loss: 0.0930\n",
      "Epoch 866/1000 Loss: 0.1705\n",
      "Epoch 867/1000 Loss: 0.0580\n",
      "Epoch 868/1000 Loss: 0.0487\n",
      "Epoch 869/1000 Loss: 0.0531\n",
      "Epoch 870/1000 Loss: 0.0550\n",
      "Epoch 871/1000 Loss: 0.1234\n",
      "Epoch 872/1000 Loss: 0.0943\n",
      "Epoch 873/1000 Loss: 0.0506\n",
      "Epoch 874/1000 Loss: 0.1349\n",
      "Epoch 875/1000 Loss: 0.0311\n",
      "Epoch 876/1000 Loss: 0.0497\n",
      "Epoch 877/1000 Loss: 0.0584\n",
      "Epoch 878/1000 Loss: 0.0610\n",
      "Epoch 879/1000 Loss: 0.0524\n",
      "Epoch 880/1000 Loss: 0.0654\n",
      "Epoch 881/1000 Loss: 0.0286\n",
      "Epoch 882/1000 Loss: 0.0136\n",
      "Epoch 883/1000 Loss: 0.1293\n",
      "Epoch 884/1000 Loss: 0.0579\n",
      "Epoch 885/1000 Loss: 0.0127\n",
      "Epoch 886/1000 Loss: 0.1045\n",
      "Epoch 887/1000 Loss: 0.0299\n",
      "Epoch 888/1000 Loss: 0.0264\n",
      "Epoch 889/1000 Loss: 0.1067\n",
      "Epoch 890/1000 Loss: 0.0567\n",
      "Epoch 891/1000 Loss: 0.0474\n",
      "Epoch 892/1000 Loss: 0.0992\n",
      "Epoch 893/1000 Loss: 0.0360\n",
      "Epoch 894/1000 Loss: 0.2926\n",
      "Epoch 895/1000 Loss: 0.0365\n",
      "Epoch 896/1000 Loss: 0.3283\n",
      "Epoch 897/1000 Loss: 0.1257\n",
      "Epoch 898/1000 Loss: 0.0932\n",
      "Epoch 899/1000 Loss: 0.1005\n",
      "Epoch 900/1000 Loss: 0.0638\n",
      "Epoch 901/1000 Loss: 0.1185\n",
      "Epoch 902/1000 Loss: 0.0373\n",
      "Epoch 903/1000 Loss: 0.1263\n",
      "Epoch 904/1000 Loss: 0.1222\n",
      "Epoch 905/1000 Loss: 0.1006\n",
      "Epoch 906/1000 Loss: 0.0869\n",
      "Epoch 907/1000 Loss: 0.0901\n",
      "Epoch 908/1000 Loss: 0.1458\n",
      "Epoch 909/1000 Loss: 0.0806\n",
      "Epoch 910/1000 Loss: 0.0568\n",
      "Epoch 911/1000 Loss: 0.0763\n",
      "Epoch 912/1000 Loss: 0.0398\n",
      "Epoch 913/1000 Loss: 0.0438\n",
      "Epoch 914/1000 Loss: 0.1398\n",
      "Epoch 915/1000 Loss: 0.0466\n",
      "Epoch 916/1000 Loss: 0.0933\n",
      "Epoch 917/1000 Loss: 0.0755\n",
      "Epoch 918/1000 Loss: 0.1006\n",
      "Epoch 919/1000 Loss: 0.0806\n",
      "Epoch 920/1000 Loss: 0.1024\n",
      "Epoch 921/1000 Loss: 0.1247\n",
      "Epoch 922/1000 Loss: 0.0920\n",
      "Epoch 923/1000 Loss: 0.1781\n",
      "Epoch 924/1000 Loss: 0.1106\n",
      "Epoch 925/1000 Loss: 0.0743\n",
      "Epoch 926/1000 Loss: 0.0626\n",
      "Epoch 927/1000 Loss: 0.1196\n",
      "Epoch 928/1000 Loss: 0.2642\n",
      "Epoch 929/1000 Loss: 0.0352\n",
      "Epoch 930/1000 Loss: 0.0451\n",
      "Epoch 931/1000 Loss: 0.0700\n",
      "Epoch 932/1000 Loss: 0.0989\n",
      "Epoch 933/1000 Loss: 0.0751\n",
      "Epoch 934/1000 Loss: 0.2653\n",
      "Epoch 935/1000 Loss: 0.0266\n",
      "Epoch 936/1000 Loss: 0.0193\n",
      "Epoch 937/1000 Loss: 0.0547\n",
      "Epoch 938/1000 Loss: 0.1100\n",
      "Epoch 939/1000 Loss: 0.0540\n",
      "Epoch 940/1000 Loss: 0.1489\n",
      "Epoch 941/1000 Loss: 0.0791\n",
      "Epoch 942/1000 Loss: 0.0462\n",
      "Epoch 943/1000 Loss: 0.1900\n",
      "Epoch 944/1000 Loss: 0.0637\n",
      "Epoch 945/1000 Loss: 0.1370\n",
      "Epoch 946/1000 Loss: 0.0612\n",
      "Epoch 947/1000 Loss: 0.0335\n",
      "Epoch 948/1000 Loss: 0.0478\n",
      "Epoch 949/1000 Loss: 0.6077\n",
      "Epoch 950/1000 Loss: 0.0428\n",
      "Epoch 951/1000 Loss: 0.0991\n",
      "Epoch 952/1000 Loss: 0.0453\n",
      "Epoch 953/1000 Loss: 0.0298\n",
      "Epoch 954/1000 Loss: 0.2826\n",
      "Epoch 955/1000 Loss: 0.0165\n",
      "Epoch 956/1000 Loss: 0.0851\n",
      "Epoch 957/1000 Loss: 0.0318\n",
      "Epoch 958/1000 Loss: 0.0819\n",
      "Epoch 959/1000 Loss: 0.1839\n",
      "Epoch 960/1000 Loss: 0.1005\n",
      "Epoch 961/1000 Loss: 0.1946\n",
      "Epoch 962/1000 Loss: 0.2685\n",
      "Epoch 963/1000 Loss: 0.0742\n",
      "Epoch 964/1000 Loss: 0.1471\n",
      "Epoch 965/1000 Loss: 0.0106\n",
      "Epoch 966/1000 Loss: 0.0319\n",
      "Epoch 967/1000 Loss: 0.1789\n",
      "Epoch 968/1000 Loss: 0.0832\n",
      "Epoch 969/1000 Loss: 0.0202\n",
      "Epoch 970/1000 Loss: 0.0923\n",
      "Epoch 971/1000 Loss: 0.2948\n",
      "Epoch 972/1000 Loss: 0.0220\n",
      "Epoch 973/1000 Loss: 0.1018\n",
      "Epoch 974/1000 Loss: 0.0307\n",
      "Epoch 975/1000 Loss: 0.0506\n",
      "Epoch 976/1000 Loss: 0.0533\n",
      "Epoch 977/1000 Loss: 0.0263\n",
      "Epoch 978/1000 Loss: 0.1959\n",
      "Epoch 979/1000 Loss: 0.1162\n",
      "Epoch 980/1000 Loss: 0.1464\n",
      "Epoch 981/1000 Loss: 0.1186\n",
      "Epoch 982/1000 Loss: 0.1550\n",
      "Epoch 983/1000 Loss: 0.1441\n",
      "Epoch 984/1000 Loss: 0.2381\n",
      "Epoch 985/1000 Loss: 0.0386\n",
      "Epoch 986/1000 Loss: 0.1286\n",
      "Epoch 987/1000 Loss: 0.0302\n",
      "Epoch 988/1000 Loss: 0.0663\n",
      "Epoch 989/1000 Loss: 0.3305\n",
      "Epoch 990/1000 Loss: 0.2143\n",
      "Epoch 991/1000 Loss: 0.2276\n",
      "Epoch 992/1000 Loss: 0.0402\n",
      "Epoch 993/1000 Loss: 0.0241\n",
      "Epoch 994/1000 Loss: 0.0710\n",
      "Epoch 995/1000 Loss: 0.0487\n",
      "Epoch 996/1000 Loss: 0.0874\n",
      "Epoch 997/1000 Loss: 0.0523\n",
      "Epoch 998/1000 Loss: 0.0254\n",
      "Epoch 999/1000 Loss: 0.0822\n",
      "Epoch 1000/1000 Loss: 0.0346\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "scheduler = DDPMScheduler(num_train_timesteps=diffusion_timesteps, beta_schedule=\"linear\")\n",
    "model = OneHotDiffusionModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "dataset = OneHotDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        # Prepare data\n",
    "        clean_data = batch.to(device)\n",
    "        noise = torch.randn_like(clean_data)\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(\n",
    "            0, scheduler.num_train_timesteps, \n",
    "            (clean_data.shape[0],), device=device\n",
    "        ).long()\n",
    "        \n",
    "        # Add noise using scheduler\n",
    "        noisy_data = scheduler.add_noise(clean_data, noise, timesteps)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = model(noisy_data, timesteps)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"one_hot_diffusion.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the Unconditional Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample statistics:\n",
      "Category 0: 5005 (50.05%)\n",
      "Category 1: 4995 (49.95%)\n",
      "Category 2: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# # Generate a sample\n",
    "# print(\"Generated sample:\", generate_sample(model, scheduler))\n",
    "\n",
    "# Generate and analyze samples\n",
    "results = generate_and_analyze_samples(model, scheduler, num_samples=10000)\n",
    "print(\"Sample statistics:\")\n",
    "for k, v in results[\"statistics\"].items():\n",
    "    print(f\"{k}: {v} ({v/100}%)\")\n",
    "\n",
    "# Example output:\n",
    "# Category 0: 253 (25.3%)\n",
    "# Category 1: 246 (24.6%)\n",
    "# Category 2: 251 (25.1%)\n",
    "# Category 3: 250 (25.0%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Train a safety policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define & Load Pretrained Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = DDPMScheduler(num_train_timesteps=diffusion_timesteps, beta_schedule=\"linear\")\n",
    "diffusion_model = OneHotDiffusionModel().to(device)\n",
    "\n",
    "checkpoint = torch.load(\"one_hot_diffusion.pth\")\n",
    "diffusion_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Safety Actor Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from diffusers import DDPMScheduler\n",
    "from collections import defaultdict\n",
    "\n",
    "class SafetyController(nn.Module):\n",
    "    \"\"\"Actor: Produces safety correction actions\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.timestep_embed = nn.Embedding(diffusion_timesteps, 32)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 32, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, dim)  # Mean\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(dim) - 1.0, requires_grad=True)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embed(t).squeeze(1)\n",
    "        x = torch.cat([x, t_embed], dim=1)\n",
    "        mean = self.net(x)\n",
    "        # print(\"X:\", x, \"Mean:\", mean)\n",
    "        return mean, self.log_std.exp()  # Return (mean, std)\n",
    "    \n",
    "def get_action(mean, std):\n",
    "    normal_dist = torch.distributions.Normal(mean, std)\n",
    "    action = normal_dist.rsample()  # Reparameterized sampling\n",
    "    return action\n",
    "\n",
    "def log_prob(mean, std, action):\n",
    "    # Add numerical stability safeguards\n",
    "    min_std = 1e-4  # Minimum standard deviation to prevent underflow\n",
    "    clamped_std = std.clamp(min=min_std)\n",
    "    \n",
    "    # Create distribution with safe parameters\n",
    "    normal_dist = torch.distributions.Normal(\n",
    "        loc=mean,\n",
    "        scale=clamped_std\n",
    "    )\n",
    "    \n",
    "    # Calculate log probs with stability\n",
    "    log_probs = normal_dist.log_prob(action)\n",
    "    \n",
    "    # Sum across action dimensions (-1)\n",
    "    return log_probs.sum(dim=-1)\n",
    "\n",
    "class SafetyCritic(nn.Module):\n",
    "    \"\"\"Critic: Predicts expected return for state-timestep pairs\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.timestep_embed = nn.Embedding(diffusion_timesteps, 32)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 32, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 1)  # Outputs value estimate\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embed(t)\n",
    "        # print(x.shape, t_embed.shape)\n",
    "        return self.net(torch.cat([x, t_embed], dim=1))\n",
    "\n",
    "# 2. Training Class -----------------------------------------------------------\n",
    "class SafetyTrainer:\n",
    "    def __init__(self, diffusion_model, scheduler, unsafe_state=[1, 0, 0]):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.diffusion_model = diffusion_model.to(self.device)\n",
    "        self.scheduler = scheduler\n",
    "        self.unsafe_state = unsafe_state\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.controller = SafetyController().to(self.device)\n",
    "        self.critic = SafetyCritic().to(self.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.ctrl_optim = optim.Adam(self.controller.parameters(), lr=1e-4)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=1e-4)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "    \n",
    "    def _is_unsafe(self, state):\n",
    "        \"\"\"Check if state is unsafe (one-hot [1,0,0])\"\"\"\n",
    "        probs = F.softmax(state, dim=-1)\n",
    "        categories = torch.argmax(probs, dim=-1)\n",
    "        one_hot_samples = F.one_hot(categories, num_classes=dim)\n",
    "        return torch.all(torch.eq(one_hot_samples.to(self.device), torch.tensor(self.unsafe_state).to(self.device)), dim=-1)\n",
    "    \n",
    "    def _compute_returns(self, rewards):\n",
    "        \"\"\"Monte Carlo returns with discounting\"\"\"\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns, device=self.device).float()\n",
    "    \n",
    "\n",
    "    def collect_trajectories(self, num_traj=100):\n",
    "        \"\"\"Batched trajectory collection\"\"\"\n",
    "        # Initialize all trajectories simultaneously\n",
    "        x = torch.randn(num_traj, dim).to(self.device)\n",
    "        batch_trajectories = defaultdict(list)\n",
    "        \n",
    "        # Store all steps for all trajectories\n",
    "        all_states = []\n",
    "        all_timesteps = []\n",
    "        all_interventions = []\n",
    "        all_value_ests = []\n",
    "        \n",
    "        for t in self.scheduler.timesteps:\n",
    "            # Process entire batch at once\n",
    "            timestep = torch.full((num_traj,), t, device=self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get actions for all trajectories\n",
    "                nominal_actions = self.diffusion_model(x, timestep)\n",
    "                safety_mean, safety_std = self.controller(x, timestep)\n",
    "                value_ests = self.critic(x, timestep)\n",
    "                \n",
    "                # Sample safety actions\n",
    "                normal_dist = torch.distributions.Normal(safety_mean, safety_std)\n",
    "                safety_actions = normal_dist.rsample()\n",
    "                \n",
    "                # Intervention decisions\n",
    "                intervene_mask = (value_ests < -0.5).squeeze()\n",
    "                \n",
    "                # Combine actions\n",
    "                actions = torch.where(intervene_mask.unsqueeze(-1), \n",
    "                                    safety_actions, \n",
    "                                    nominal_actions)\n",
    "                \n",
    "                # Step diffusion process\n",
    "                x = self.scheduler.step(actions, t, x).prev_sample\n",
    "                \n",
    "            # Store batch data\n",
    "            all_states.append(x.clone())\n",
    "            all_timesteps.append(timestep.clone())\n",
    "            all_interventions.append(intervene_mask)\n",
    "            all_value_ests.append(value_ests)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(all_states)  # [num_steps, batch_size, dim]\n",
    "        timesteps = torch.stack(all_timesteps)  # [num_steps, batch_size]\n",
    "        interventions = torch.stack(all_interventions)  # [num_steps, batch_size]\n",
    "        value_ests = torch.stack(all_value_ests)  # [num_steps, batch_size, 1]\n",
    "        \n",
    "        # Get final rewards\n",
    "        final_unsafe = self._is_unsafe(x)\n",
    "        final_rewards = -final_unsafe.float().squeeze()\n",
    "        \n",
    "        # Reorganize into individual trajectories\n",
    "        trajs = []\n",
    "        for i in range(num_traj):\n",
    "            transitions = []\n",
    "            for step in range(len(self.scheduler.timesteps)):\n",
    "                transitions.append({\n",
    "                    'state': states[step, i],\n",
    "                    'timestep': timesteps[step, i],\n",
    "                    'intervened': interventions[step, i].item(),\n",
    "                    'value_est': value_ests[step, i]\n",
    "                })\n",
    "            # Assign final reward to last step\n",
    "            transitions[-1]['reward'] = final_rewards[i].item()\n",
    "            \n",
    "            trajs.append({\n",
    "                'transitions': transitions,\n",
    "                'reward': final_rewards[i].item()\n",
    "            })\n",
    "        \n",
    "        return trajs\n",
    "    \n",
    "\n",
    "    def train(self, epochs=100, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            # Collect trajectories (with complete state/timestep records)\n",
    "            trajs = self.collect_trajectories(batch_size)\n",
    "            \n",
    "            # Process trajectories\n",
    "            all_states = []\n",
    "            all_timesteps = []\n",
    "            all_returns = []\n",
    "            all_interventions = []\n",
    "            \n",
    "            for traj in trajs:\n",
    "                # Compute returns (non-differentiable)\n",
    "                rewards = [0] * (len(traj['transitions']) - 1) + [traj['reward']]\n",
    "                returns = self._compute_returns(rewards)\n",
    "                \n",
    "                # Collect states and timesteps\n",
    "                for i, transition in enumerate(traj['transitions']):\n",
    "                    all_states.append(transition['state'].squeeze(0))\n",
    "                    all_timesteps.append(transition['timestep'].squeeze(0))\n",
    "                    all_returns.append(returns[i])\n",
    "                    all_interventions.append(transition['intervened'])\n",
    "            \n",
    "            # Convert to tensors\n",
    "            states = torch.stack(all_states)\n",
    "            timesteps = torch.stack(all_timesteps)\n",
    "            returns = torch.stack(all_returns)\n",
    "            \n",
    "            # 1. Train Critic (differentiable forward pass)\n",
    "            current_values = self.critic(states, timesteps).squeeze()\n",
    "            print(current_values, returns)\n",
    "            critic_loss = F.mse_loss(current_values, returns)\n",
    "            \n",
    "            self.critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optim.step()\n",
    "\n",
    "            loss = 0\n",
    "            \n",
    "            # 2. Train Controller (only on intervention points)\n",
    "            if sum(all_interventions) > 0:\n",
    "                # Filter to intervention points\n",
    "                intv_mask = torch.tensor(all_interventions, device=self.device)\n",
    "                intv_states = states[intv_mask]\n",
    "                intv_timesteps = timesteps[intv_mask]\n",
    "                intv_returns = returns[intv_mask]\n",
    "                \n",
    "                # Calculate advantages\n",
    "                with torch.no_grad():\n",
    "                    intv_values = self.critic(intv_states, intv_timesteps).squeeze()\n",
    "                advantages = intv_returns - intv_values   # If the safe policy increase the return?\n",
    "                # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                # Policy gradient update\n",
    "                mean, std = self.controller(intv_states, intv_timesteps)\n",
    "                actions = get_action(mean, std)\n",
    "                log_probs = log_prob(mean, std, actions)\n",
    "\n",
    "                loss = -(advantages * log_probs).mean()  # policy gradient\n",
    "                \n",
    "                # Add after backward():\n",
    "                torch.nn.utils.clip_grad_norm_(self.controller.parameters(), 1.0)\n",
    "                self.ctrl_optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.ctrl_optim.step()\n",
    "            \n",
    "            # Logging\n",
    "            unsafe_rate = sum(t['reward'] for t in trajs) / batch_size\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Critic Loss: {critic_loss} | Controller Loss: {loss} | Unsafe Rate: {-unsafe_rate:.3f}\")\n",
    "\n",
    "\n",
    "# 3. Usage ---------------------------------------------------------------------\n",
    "# Initialize components\n",
    "trainer = SafetyTrainer(diffusion_model, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trainer.state_dict(), \"safe_controller.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0056, -0.0435,  0.0446,  ..., -0.0266, -0.0633, -0.0434],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 1/100 | Critic Loss: 0.2681463956832886 | Controller Loss: 0 | Unsafe Rate: 0.562\n",
      "tensor([-0.0025, -0.0554,  0.0422,  ..., -0.0332, -0.0656, -0.0504],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 2/100 | Critic Loss: 0.25165829062461853 | Controller Loss: 0 | Unsafe Rate: 0.539\n",
      "tensor([-0.0200, -0.0719,  0.0260,  ..., -0.0401, -0.0681, -0.0574],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 3/100 | Critic Loss: 0.1933407336473465 | Controller Loss: 0 | Unsafe Rate: 0.422\n",
      "tensor([-0.0360, -0.0781,  0.0066,  ..., -0.0471, -0.0705, -0.0644],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 4/100 | Critic Loss: 0.21046842634677887 | Controller Loss: 0 | Unsafe Rate: 0.469\n",
      "tensor([-0.0233, -0.0753,  0.0151,  ..., -0.0589, -0.0740, -0.0811],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 5/100 | Critic Loss: 0.2573547959327698 | Controller Loss: 0 | Unsafe Rate: 0.586\n",
      "tensor([-0.0318, -0.0841,  0.0071,  ..., -0.0661, -0.0775, -0.0889],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 6/100 | Critic Loss: 0.2387417107820511 | Controller Loss: 0 | Unsafe Rate: 0.555\n",
      "tensor([-0.0788, -0.1124, -0.0334,  ..., -0.0673, -0.0776, -0.0852],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 7/100 | Critic Loss: 0.17211851477622986 | Controller Loss: 0 | Unsafe Rate: 0.406\n",
      "tensor([-0.0622, -0.0949, -0.0213,  ..., -0.0737, -0.0799, -0.0920],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 8/100 | Critic Loss: 0.21054387092590332 | Controller Loss: 0 | Unsafe Rate: 0.508\n",
      "tensor([-0.0771, -0.1176, -0.0402,  ..., -0.0879, -0.0884, -0.1122],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 9/100 | Critic Loss: 0.202596053481102 | Controller Loss: 0 | Unsafe Rate: 0.500\n",
      "tensor([-0.0936, -0.1034, -0.0621,  ..., -0.0870, -0.0846, -0.1054],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 10/100 | Critic Loss: 0.1924527883529663 | Controller Loss: 0 | Unsafe Rate: 0.484\n",
      "tensor([-0.0960, -0.1401, -0.0624,  ..., -0.0936, -0.0870, -0.1120],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 11/100 | Critic Loss: 0.17069633305072784 | Controller Loss: 0 | Unsafe Rate: 0.438\n",
      "tensor([-0.0687, -0.0936, -0.0422,  ..., -0.0993, -0.0892, -0.1185],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 12/100 | Critic Loss: 0.17903879284858704 | Controller Loss: 0 | Unsafe Rate: 0.469\n",
      "tensor([-0.1318, -0.1355, -0.0931,  ..., -0.1060, -0.0915, -0.1248],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 13/100 | Critic Loss: 0.1788289099931717 | Controller Loss: 0 | Unsafe Rate: 0.477\n",
      "tensor([-0.0771, -0.1272, -0.0486,  ..., -0.1116, -0.0938, -0.1311],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 14/100 | Critic Loss: 0.14692959189414978 | Controller Loss: 0 | Unsafe Rate: 0.398\n",
      "tensor([-0.0660, -0.1035, -0.0400,  ..., -0.1178, -0.0961, -0.1371],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 15/100 | Critic Loss: 0.17460648715496063 | Controller Loss: 0 | Unsafe Rate: 0.484\n",
      "tensor([-0.1086, -0.1397, -0.1002,  ..., -0.1377, -0.1149, -0.1649],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 16/100 | Critic Loss: 0.20050674676895142 | Controller Loss: 0 | Unsafe Rate: 0.570\n",
      "tensor([-0.1199, -0.1679, -0.1035,  ..., -0.1442, -0.1189, -0.1724],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 17/100 | Critic Loss: 0.16834987699985504 | Controller Loss: 0 | Unsafe Rate: 0.484\n",
      "tensor([-0.1355, -0.1885, -0.1141,  ..., -0.1511, -0.1229, -0.1798],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 18/100 | Critic Loss: 0.17787112295627594 | Controller Loss: 0 | Unsafe Rate: 0.523\n",
      "tensor([-0.1363, -0.1755, -0.1132,  ..., -0.1582, -0.1268, -0.1871],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 19/100 | Critic Loss: 0.17504093050956726 | Controller Loss: 0 | Unsafe Rate: 0.523\n",
      "tensor([-0.1219, -0.1482, -0.1056,  ..., -0.1474, -0.1082, -0.1675],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 20/100 | Critic Loss: 0.15310038626194 | Controller Loss: 0 | Unsafe Rate: 0.461\n",
      "tensor([-0.1288, -0.1949, -0.1167,  ..., -0.1723, -0.1352, -0.2019],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 21/100 | Critic Loss: 0.16093003749847412 | Controller Loss: 0 | Unsafe Rate: 0.500\n",
      "tensor([-0.1175, -0.2141, -0.1124,  ..., -0.1584, -0.1133, -0.1792],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 22/100 | Critic Loss: 0.16308356821537018 | Controller Loss: 0 | Unsafe Rate: 0.516\n",
      "tensor([-0.1588, -0.2193, -0.1336,  ..., -0.1634, -0.1155, -0.1848],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 23/100 | Critic Loss: 0.14155836403369904 | Controller Loss: 0 | Unsafe Rate: 0.453\n",
      "tensor([-0.1017, -0.1464, -0.0896,  ..., -0.1700, -0.1182, -0.1902],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 24/100 | Critic Loss: 0.15596339106559753 | Controller Loss: 0 | Unsafe Rate: 0.516\n",
      "tensor([-0.0977, -0.1402, -0.0797,  ..., -0.1745, -0.1204, -0.1955],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 25/100 | Critic Loss: 0.17123469710350037 | Controller Loss: 0 | Unsafe Rate: 0.578\n",
      "tensor([-0.1495, -0.1976, -0.1380,  ..., -0.1813, -0.1239, -0.2013],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 26/100 | Critic Loss: 0.14377200603485107 | Controller Loss: 0 | Unsafe Rate: 0.484\n",
      "tensor([-0.1279, -0.1589, -0.1073,  ..., -0.2116, -0.1606, -0.2441],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 27/100 | Critic Loss: 0.1328481286764145 | Controller Loss: 0 | Unsafe Rate: 0.453\n",
      "tensor([-0.2395, -0.2904, -0.2290,  ..., -0.2190, -0.1650, -0.2508],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 28/100 | Critic Loss: 0.14686307311058044 | Controller Loss: 0 | Unsafe Rate: 0.523\n",
      "tensor([-0.1379, -0.1998, -0.1336,  ..., -0.1953, -0.1311, -0.2165],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 29/100 | Critic Loss: 0.13022084534168243 | Controller Loss: 0 | Unsafe Rate: 0.461\n",
      "tensor([-0.1948, -0.2295, -0.1867,  ..., -0.2317, -0.1744, -0.2640],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 30/100 | Critic Loss: 0.13697503507137299 | Controller Loss: 0 | Unsafe Rate: 0.500\n",
      "tensor([-0.2328, -0.2718, -0.2212,  ..., -0.2047, -0.1365, -0.2258],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 31/100 | Critic Loss: 0.1407989263534546 | Controller Loss: 0 | Unsafe Rate: 0.531\n",
      "tensor([-0.2120, -0.2424, -0.1972,  ..., -0.2428, -0.1832, -0.2767],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 32/100 | Critic Loss: 0.13435222208499908 | Controller Loss: 0 | Unsafe Rate: 0.508\n",
      "tensor([-0.2066, -0.2733, -0.1973,  ..., -0.2496, -0.1885, -0.2831],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 33/100 | Critic Loss: 0.12469156086444855 | Controller Loss: 0 | Unsafe Rate: 0.469\n",
      "tensor([-0.1903, -0.2529, -0.1758,  ..., -0.2186, -0.1448, -0.2387],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 34/100 | Critic Loss: 0.12946033477783203 | Controller Loss: 0 | Unsafe Rate: 0.508\n",
      "tensor([-0.2898, -0.3463, -0.2871,  ..., -0.2620, -0.1978, -0.2950],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 35/100 | Critic Loss: 0.12769097089767456 | Controller Loss: 0 | Unsafe Rate: 0.500\n",
      "tensor([-0.1892, -0.2424, -0.1883,  ..., -0.2262, -0.1498, -0.2461],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 36/100 | Critic Loss: 0.11940015852451324 | Controller Loss: 0 | Unsafe Rate: 0.477\n",
      "tensor([-0.1769, -0.1677, -0.1575,  ..., -0.2302, -0.1528, -0.2496],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 37/100 | Critic Loss: 0.12513446807861328 | Controller Loss: 0 | Unsafe Rate: 0.508\n",
      "tensor([-0.2747, -0.3466, -0.2645,  ..., -0.2782, -0.2135, -0.3121],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 38/100 | Critic Loss: 0.13283991813659668 | Controller Loss: 0 | Unsafe Rate: 0.570\n",
      "tensor([-0.2711, -0.3084, -0.2742,  ..., -0.2384, -0.1590, -0.2566],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 39/100 | Critic Loss: 0.11756999045610428 | Controller Loss: 0 | Unsafe Rate: 0.492\n",
      "tensor([-0.3049, -0.4014, -0.2997,  ..., -0.2901, -0.2239, -0.3231],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 40/100 | Critic Loss: 0.12093762308359146 | Controller Loss: 0 | Unsafe Rate: 0.516\n",
      "tensor([-0.2541, -0.2920, -0.2378,  ..., -0.2435, -0.1644, -0.2625],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 41/100 | Critic Loss: 0.1279028058052063 | Controller Loss: 0 | Unsafe Rate: 0.570\n",
      "tensor([-0.2999, -0.3290, -0.2893,  ..., -0.2998, -0.2327, -0.3332],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 42/100 | Critic Loss: 0.12248001992702484 | Controller Loss: 0 | Unsafe Rate: 0.555\n",
      "tensor([-0.2223, -0.2413, -0.2016,  ..., -0.2523, -0.1703, -0.2686],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 43/100 | Critic Loss: 0.10323764383792877 | Controller Loss: 0.24951106309890747 | Unsafe Rate: 0.438\n",
      "tensor([-0.2860, -0.3391, -0.2681,  ..., -0.2542, -0.1731, -0.2710],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 44/100 | Critic Loss: 0.11096629500389099 | Controller Loss: 1.3351677656173706 | Unsafe Rate: 0.492\n",
      "tensor([-0.1959, -0.2232, -0.2034,  ..., -0.3171, -0.2498, -0.3485],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 45/100 | Critic Loss: 0.10807722061872482 | Controller Loss: -0.5267872214317322 | Unsafe Rate: 0.492\n",
      "tensor([-0.2429, -0.2257, -0.2389,  ..., -0.3221, -0.2558, -0.3531],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 46/100 | Critic Loss: 0.10224854201078415 | Controller Loss: 0.1356193572282791 | Unsafe Rate: 0.445\n",
      "tensor([-0.2447, -0.2834, -0.2515,  ..., -0.2637, -0.1817, -0.2768],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 47/100 | Critic Loss: 0.1055726706981659 | Controller Loss: -0.07983854413032532 | Unsafe Rate: 0.492\n",
      "tensor([-0.3474, -0.3861, -0.3288,  ..., -0.2664, -0.1850, -0.2783],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 48/100 | Critic Loss: 0.11010946333408356 | Controller Loss: -0.3888286352157593 | Unsafe Rate: 0.531\n",
      "tensor([-0.2857, -0.3459, -0.2678,  ..., -0.2649, -0.1873, -0.2794],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 49/100 | Critic Loss: 0.11305227130651474 | Controller Loss: 0.23961377143859863 | Unsafe Rate: 0.578\n",
      "tensor([-0.3202, -0.3766, -0.3095,  ..., -0.2679, -0.1897, -0.2806],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 50/100 | Critic Loss: 0.10576923936605453 | Controller Loss: -0.0034985989332199097 | Unsafe Rate: 0.516\n",
      "tensor([-0.3067, -0.3179, -0.2955,  ..., -0.3411, -0.2815, -0.3727],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 51/100 | Critic Loss: 0.10083730518817902 | Controller Loss: -0.09075392782688141 | Unsafe Rate: 0.492\n",
      "tensor([-0.3487, -0.3800, -0.3465,  ..., -0.2718, -0.1955, -0.2829],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 52/100 | Critic Loss: 0.09297721832990646 | Controller Loss: -0.14206673204898834 | Unsafe Rate: 0.430\n",
      "tensor([-0.3673, -0.3607, -0.3197,  ..., -0.2734, -0.1984, -0.2835],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 53/100 | Critic Loss: 0.09786178171634674 | Controller Loss: 0.009006251581013203 | Unsafe Rate: 0.484\n",
      "tensor([-0.3233, -0.3358, -0.2795,  ..., -0.2742, -0.2008, -0.2835],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 54/100 | Critic Loss: 0.10512830317020416 | Controller Loss: 0.029014641419053078 | Unsafe Rate: 0.570\n",
      "tensor([-0.2607, -0.2831, -0.2530,  ..., -0.3561, -0.3038, -0.3860],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 55/100 | Critic Loss: 0.09768088161945343 | Controller Loss: 0.13448230922222137 | Unsafe Rate: 0.516\n",
      "tensor([-0.3039, -0.3291, -0.3067,  ..., -0.3592, -0.3101, -0.3894],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 56/100 | Critic Loss: 0.08734762668609619 | Controller Loss: 0.13613447546958923 | Unsafe Rate: 0.383\n",
      "tensor([-0.3027, -0.3127, -0.2896,  ..., -0.3660, -0.3168, -0.3919],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 57/100 | Critic Loss: 0.10025215148925781 | Controller Loss: -0.28699570894241333 | Unsafe Rate: 0.562\n",
      "tensor([-0.4542, -0.4781, -0.4520,  ..., -0.3681, -0.3196, -0.3939],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 58/100 | Critic Loss: 0.09129934012889862 | Controller Loss: -0.15683723986148834 | Unsafe Rate: 0.469\n",
      "tensor([-0.5191, -0.5458, -0.5239,  ..., -0.3709, -0.3267, -0.3970],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 59/100 | Critic Loss: 0.09594126790761948 | Controller Loss: 0.11438573151826859 | Unsafe Rate: 0.531\n",
      "tensor([-0.3399, -0.3766, -0.3610,  ..., -0.3745, -0.3325, -0.3994],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 60/100 | Critic Loss: 0.0939682275056839 | Controller Loss: 0.06769725680351257 | Unsafe Rate: 0.508\n",
      "tensor([-0.4359, -0.4457, -0.4097,  ..., -0.2768, -0.2201, -0.2821],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 61/100 | Critic Loss: 0.09002537280321121 | Controller Loss: 0.013647982850670815 | Unsafe Rate: 0.469\n",
      "tensor([-0.3238, -0.2825, -0.2975,  ..., -0.3815, -0.3426, -0.4038],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 62/100 | Critic Loss: 0.08978711813688278 | Controller Loss: -0.04779869690537453 | Unsafe Rate: 0.469\n",
      "tensor([-0.2121, -0.1630, -0.1798,  ..., -0.2773, -0.2225, -0.2795],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 63/100 | Critic Loss: 0.08259057998657227 | Controller Loss: -0.07893158495426178 | Unsafe Rate: 0.383\n",
      "tensor([-0.1931, -0.1722, -0.1491,  ..., -0.3839, -0.3535, -0.4073],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 64/100 | Critic Loss: 0.08994626998901367 | Controller Loss: 0.25923725962638855 | Unsafe Rate: 0.492\n",
      "tensor([-0.3610, -0.3659, -0.3428,  ..., -0.2755, -0.2275, -0.2765],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 65/100 | Critic Loss: 0.08377315104007721 | Controller Loss: -0.019914690405130386 | Unsafe Rate: 0.430\n",
      "tensor([-0.3318, -0.3323, -0.3219,  ..., -0.2731, -0.2296, -0.2745],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 66/100 | Critic Loss: 0.09075870364904404 | Controller Loss: 0.09691775590181351 | Unsafe Rate: 0.570\n",
      "tensor([-0.2998, -0.2963, -0.2958,  ..., -0.3901, -0.3689, -0.4113],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 67/100 | Critic Loss: 0.08939053118228912 | Controller Loss: 0.02091110683977604 | Unsafe Rate: 0.500\n",
      "tensor([-0.2313, -0.1687, -0.2100,  ..., -0.2737, -0.2335, -0.2713],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 68/100 | Critic Loss: 0.08818702399730682 | Controller Loss: 0.08621202409267426 | Unsafe Rate: 0.523\n",
      "tensor([-0.3246, -0.2782, -0.2943,  ..., -0.2698, -0.2358, -0.2698],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 69/100 | Critic Loss: 0.08269109576940536 | Controller Loss: 0.01294777449220419 | Unsafe Rate: 0.484\n",
      "tensor([-0.2992, -0.2982, -0.3074,  ..., -0.3989, -0.3835, -0.4157],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 70/100 | Critic Loss: 0.08670224249362946 | Controller Loss: -0.001248265616595745 | Unsafe Rate: 0.484\n",
      "tensor([-0.4368, -0.4027, -0.3933,  ..., -0.3995, -0.3863, -0.4167],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 71/100 | Critic Loss: 0.08489169925451279 | Controller Loss: 0.09230174869298935 | Unsafe Rate: 0.516\n",
      "tensor([-0.3890, -0.4093, -0.3958,  ..., -0.4030, -0.3919, -0.4187],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 72/100 | Critic Loss: 0.08400727808475494 | Controller Loss: -0.04651026800274849 | Unsafe Rate: 0.547\n",
      "tensor([-0.2881, -0.2442, -0.2490,  ..., -0.4051, -0.3959, -0.4202],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 73/100 | Critic Loss: 0.08650533854961395 | Controller Loss: 0.0015418140683323145 | Unsafe Rate: 0.570\n",
      "tensor([-0.3091, -0.2886, -0.2980,  ..., -0.4058, -0.4009, -0.4223],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 74/100 | Critic Loss: 0.08085668087005615 | Controller Loss: -0.04564483091235161 | Unsafe Rate: 0.469\n",
      "tensor([-0.4864, -0.4219, -0.4585,  ..., -0.4083, -0.4070, -0.4246],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 75/100 | Critic Loss: 0.08137668669223785 | Controller Loss: 0.011584066785871983 | Unsafe Rate: 0.539\n",
      "tensor([-0.2930, -0.3063, -0.3119,  ..., -0.4059, -0.4091, -0.4259],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 76/100 | Critic Loss: 0.0808897316455841 | Controller Loss: 0.09188619256019592 | Unsafe Rate: 0.492\n",
      "tensor([-0.2917, -0.2481, -0.2496,  ..., -0.2647, -0.2487, -0.2592],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 77/100 | Critic Loss: 0.07667342573404312 | Controller Loss: -0.03807903453707695 | Unsafe Rate: 0.492\n",
      "tensor([-0.0526,  0.0155, -0.0243,  ..., -0.4127, -0.4187, -0.4299],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 78/100 | Critic Loss: 0.07520908862352371 | Controller Loss: 0.08931831270456314 | Unsafe Rate: 0.430\n",
      "tensor([-0.3145, -0.2815, -0.2725,  ..., -0.4136, -0.4234, -0.4321],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 79/100 | Critic Loss: 0.07503282278776169 | Controller Loss: -0.011140815913677216 | Unsafe Rate: 0.484\n",
      "tensor([-0.5729, -0.5917, -0.5972,  ..., -0.2618, -0.2524, -0.2551],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 80/100 | Critic Loss: 0.07558596134185791 | Controller Loss: -0.007202063221484423 | Unsafe Rate: 0.523\n",
      "tensor([-0.3113, -0.2754, -0.3076,  ..., -0.2597, -0.2554, -0.2544],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 81/100 | Critic Loss: 0.07641768455505371 | Controller Loss: 0.03349793702363968 | Unsafe Rate: 0.484\n",
      "tensor([-0.2618, -0.2093, -0.2404,  ..., -0.2628, -0.2574, -0.2533],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 82/100 | Critic Loss: 0.07924102246761322 | Controller Loss: 0.08427570015192032 | Unsafe Rate: 0.547\n",
      "tensor([-0.2347, -0.2088, -0.2264,  ..., -0.4249, -0.4412, -0.4397],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 83/100 | Critic Loss: 0.0779033750295639 | Controller Loss: -0.006846248637884855 | Unsafe Rate: 0.562\n",
      "tensor([-0.3845, -0.3189, -0.3589,  ..., -0.2639, -0.2600, -0.2515],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 84/100 | Critic Loss: 0.07299386709928513 | Controller Loss: 0.004989051725715399 | Unsafe Rate: 0.547\n",
      "tensor([-0.3286, -0.2817, -0.2770,  ..., -0.4275, -0.4481, -0.4443],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 85/100 | Critic Loss: 0.0728219598531723 | Controller Loss: -0.11419659852981567 | Unsafe Rate: 0.406\n",
      "tensor([-0.1943, -0.2116, -0.1698,  ..., -0.2577, -0.2617, -0.2500],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 86/100 | Critic Loss: 0.07477369159460068 | Controller Loss: 0.003284918610006571 | Unsafe Rate: 0.477\n",
      "tensor([-0.5375, -0.5437, -0.5358,  ..., -0.2649, -0.2672, -0.2511],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 87/100 | Critic Loss: 0.07203519344329834 | Controller Loss: 0.05360281839966774 | Unsafe Rate: 0.477\n",
      "tensor([-0.2606, -0.2566, -0.2641,  ..., -0.4391, -0.4613, -0.4515],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 88/100 | Critic Loss: 0.07100017368793488 | Controller Loss: 0.020436564460396767 | Unsafe Rate: 0.531\n",
      "tensor([-0.5806, -0.6084, -0.5751,  ..., -0.4378, -0.4653, -0.4537],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 89/100 | Critic Loss: 0.07374126464128494 | Controller Loss: -0.05153435468673706 | Unsafe Rate: 0.531\n",
      "tensor([-0.4580, -0.4003, -0.4062,  ..., -0.2548, -0.2662, -0.2470],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 90/100 | Critic Loss: 0.0702066496014595 | Controller Loss: -0.026809491217136383 | Unsafe Rate: 0.539\n",
      "tensor([ 0.0287,  0.0442,  0.0467,  ..., -0.2537, -0.2643, -0.2453],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 91/100 | Critic Loss: 0.06775002181529999 | Controller Loss: 0.03433173894882202 | Unsafe Rate: 0.453\n",
      "tensor([-0.2584, -0.2310, -0.2071,  ..., -0.4464, -0.4761, -0.4610],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 92/100 | Critic Loss: 0.07134603708982468 | Controller Loss: 2.5810079023358412e-05 | Unsafe Rate: 0.500\n",
      "tensor([-0.1388, -0.1006, -0.1267,  ..., -0.2593, -0.2664, -0.2438],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 93/100 | Critic Loss: 0.06753114610910416 | Controller Loss: 0.020685413852334023 | Unsafe Rate: 0.516\n",
      "tensor([-0.4598, -0.4455, -0.4786,  ..., -0.4471, -0.4834, -0.4656],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 94/100 | Critic Loss: 0.06559739261865616 | Controller Loss: -0.014858217909932137 | Unsafe Rate: 0.484\n",
      "tensor([ 0.0097,  0.0994,  0.0301,  ..., -0.2552, -0.2675, -0.2426],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 95/100 | Critic Loss: 0.06869029253721237 | Controller Loss: 0.004343321081250906 | Unsafe Rate: 0.500\n",
      "tensor([-0.3697, -0.3654, -0.3618,  ..., -0.2523, -0.2689, -0.2423],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 96/100 | Critic Loss: 0.07134171575307846 | Controller Loss: -0.04353024438023567 | Unsafe Rate: 0.570\n",
      "tensor([-0.5021, -0.4737, -0.5138,  ..., -0.2509, -0.2723, -0.2430],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 97/100 | Critic Loss: 0.06838218867778778 | Controller Loss: 0.020499741658568382 | Unsafe Rate: 0.500\n",
      "tensor([-0.2621, -0.2137, -0.2344,  ..., -0.4639, -0.5025, -0.4775],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 98/100 | Critic Loss: 0.06547597795724869 | Controller Loss: -0.016404470428824425 | Unsafe Rate: 0.570\n",
      "tensor([-0.2340, -0.2179, -0.2579,  ..., -0.4667, -0.5062, -0.4806],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 99/100 | Critic Loss: 0.06891836225986481 | Controller Loss: 0.005817172583192587 | Unsafe Rate: 0.531\n",
      "tensor([-0.4290, -0.3854, -0.4034,  ..., -0.4601, -0.4996, -0.4802],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.3697, -0.3735, -0.3772,  ..., -0.9801, -0.9900, -1.0000],\n",
      "       device='cuda:0')\n",
      "Epoch 100/100 | Critic Loss: 0.06214391440153122 | Controller Loss: 0.0307430662214756 | Unsafe Rate: 0.430\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer.train(epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(self, epochs=100, batch_size=32):\n",
    "    #     for epoch in range(epochs):\n",
    "    #         # Collect trajectories\n",
    "    #         trajs = self.collect_trajectories(batch_size)\n",
    "            \n",
    "    #         # Process trajectories\n",
    "    #         all_returns = []\n",
    "    #         all_values = []\n",
    "    #         all_actions = []\n",
    "            \n",
    "    #         for traj in trajs:\n",
    "    #             # Compute returns for each timestep\n",
    "    #             rewards = [traj['reward']] * len(traj['transitions'])\n",
    "    #             returns = self._compute_returns(rewards)\n",
    "                \n",
    "    #             # Collect training data\n",
    "    #             for i, transition in enumerate(traj['transitions']):\n",
    "    #                 all_returns.append(returns[i])\n",
    "    #                 all_values.append(transition['value_est'])\n",
    "    #                 if transition['intervened']:\n",
    "    #                     all_actions.append((\n",
    "    #                         transition['state'],\n",
    "    #                         transition['timestep']\n",
    "    #                     ))\n",
    "            \n",
    "    #         # Convert to tensors (handle empty cases)\n",
    "    #         returns_tensor = torch.stack(all_returns) if all_returns else None\n",
    "    #         values_tensor = torch.stack(all_values) if all_values else None\n",
    "            \n",
    "    #         # 1. Always train critic if we have any data\n",
    "    #         if returns_tensor is not None and values_tensor is not None:\n",
    "    #             critic_loss = F.mse_loss(values_tensor.squeeze(), returns_tensor)\n",
    "    #             self.critic_optim.zero_grad()\n",
    "    #             critic_loss.backward()\n",
    "    #             self.critic_optim.step()\n",
    "    #         else:\n",
    "    #             print(\"No data for critic training\")\n",
    "    #             continue\n",
    "                \n",
    "    #         # 2. Only train controller if interventions occurred\n",
    "    #         if len(all_actions) > 0:\n",
    "    #             states_tensor = torch.stack([a[0] for a in all_actions])\n",
    "    #             timesteps_tensor = torch.stack([a[1] for a in all_actions])\n",
    "                \n",
    "    #             # Calculate advantages\n",
    "    #             advantages = returns_tensor - values_tensor.detach()\n",
    "                \n",
    "    #             # Get policy outputs for intervention points\n",
    "    #             actions_pred = self.controller(states_tensor, timesteps_tensor)\n",
    "                \n",
    "    #             # Calculate loss\n",
    "    #             loss = -(advantages * torch.log(F.softmax(actions_pred, dim=-1))).mean()\n",
    "                \n",
    "    #             self.ctrl_optim.zero_grad()\n",
    "    #             loss.backward()\n",
    "    #             self.ctrl_optim.step()\n",
    "    #         else:\n",
    "    #             print(\"No interventions - controller not updated\")\n",
    "            \n",
    "    #         # Logging\n",
    "    #         unsafe_rate = sum(t['reward'] for t in trajs) / batch_size\n",
    "    #         print(f\"Epoch {epoch+1}/{epochs} | Unsafe Rate: {-unsafe_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deploy the safe controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment\n",
    "def safe_generate(diffusion_model, safety_controller, critic, scheduler, bsz=1):\n",
    "    x = torch.randn(bsz, dim).to(device)\n",
    "    for t in scheduler.timesteps:\n",
    "        timestep = torch.full((bsz,), t, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get actions for all samples in batch\n",
    "            nominal = diffusion_model(x, timestep)\n",
    "            safety, std = safety_controller(x, timestep)\n",
    "            values = critic(x, timestep)  # Shape: [bsz, 1]\n",
    "            \n",
    "        # Create intervention mask for entire batch\n",
    "        intervene_mask = (values < -0.5).squeeze(-1)  # [bsz]\n",
    "        \n",
    "        # Combine actions using mask\n",
    "        actions = torch.where(\n",
    "            intervene_mask.unsqueeze(-1),  # [bsz, 1]\n",
    "            safety,\n",
    "            nominal\n",
    "        )\n",
    "        \n",
    "        # Update all samples in batch simultaneously\n",
    "        x = scheduler.step(actions, t, x).prev_sample\n",
    "    \n",
    "    # Convert to one-hot and return\n",
    "    return F.one_hot(x.argmax(-1), num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.controller.load_state_dict(torch.load(\"latest_safe_controller.pth\"))\n",
    "trainer.critic.load_state_dict(torch.load(\"latest_safe_critic.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample statistics:\n",
      "Category 0: 4925 (49.25%)\n",
      "Category 1: 5075 (50.75%)\n",
      "Category 2: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Generate and analyze samples\n",
    "results = generate_and_analyze_samples(diffusion_model, scheduler, num_samples=10000)\n",
    "print(\"Sample statistics:\")\n",
    "for k, v in results[\"statistics\"].items():\n",
    "    print(f\"{k}: {v} ({v/100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 4])\n",
      "Sample statistics:\n",
      "Category 0: 4424 (44.24%)\n",
      "Category 1: 5437 (54.37%)\n",
      "Category 2: 139 (1.39%)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "categories = []\n",
    "categories = safe_generate(diffusion_model, trainer.controller, trainer.critic, scheduler, bsz=10000)\n",
    "\n",
    "print(categories.shape)\n",
    "# categories = torch.stack(categories)\n",
    "counts = torch.bincount(torch.argmax(categories, dim=-1), minlength=3)\n",
    "stats = {\n",
    "    f\"Category {i}\": count.item() \n",
    "    for i, count in enumerate(counts)\n",
    "}\n",
    "\n",
    "print(\"Sample statistics:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v} ({v/100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.63it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_critic_data(safety_trainer, num_trajectories=10000):\n",
    "    \"\"\"Collect states, critic values, and final outcomes\"\"\"\n",
    "    all_states = []\n",
    "    all_timesteps = []\n",
    "    all_values = []\n",
    "    all_unsafe = []\n",
    "    \n",
    "    device = safety_trainer.device\n",
    "    \n",
    "    # Generate in batches for efficiency\n",
    "    batch_size = 256\n",
    "    num_batches = int(np.ceil(num_trajectories / batch_size))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(num_batches)):\n",
    "            # Generate batch\n",
    "            x = torch.randn(batch_size, dim).to(device)\n",
    "            batch_states = []\n",
    "            batch_values = []\n",
    "            \n",
    "            # Run through diffusion process\n",
    "            for t in safety_trainer.scheduler.timesteps:\n",
    "                timestep = torch.full((batch_size,), t, device=device)\n",
    "                \n",
    "                # Store states and values\n",
    "                value = safety_trainer.critic(x, timestep)\n",
    "                batch_states.append(x.cpu().numpy())\n",
    "                batch_values.append(value.cpu().numpy())\n",
    "                \n",
    "                # Step diffusion\n",
    "                nominal = safety_trainer.diffusion_model(x, timestep)\n",
    "                x = safety_trainer.scheduler.step(nominal, t, x).prev_sample\n",
    "            \n",
    "            # Get final outcomes\n",
    "            final_unsafe = safety_trainer._is_unsafe(x).cpu().numpy()\n",
    "            \n",
    "            # Reorganize data\n",
    "            for i in range(batch_size):\n",
    "                all_states.extend([batch_states[t][i] for t in range(len(safety_trainer.scheduler.timesteps))])\n",
    "                all_timesteps.extend(safety_trainer.scheduler.timesteps)\n",
    "                all_values.extend([batch_values[t][i] for t in range(len(safety_trainer.scheduler.timesteps))])\n",
    "                all_unsafe.extend([final_unsafe[i]] * len(safety_trainer.scheduler.timesteps))\n",
    "    \n",
    "    return {\n",
    "        'states': np.array(all_states),\n",
    "        'timesteps': np.array(all_timesteps),\n",
    "        'values': np.array(all_values).squeeze(),\n",
    "        'unsafe': np.array(all_unsafe)\n",
    "    }\n",
    "\n",
    "def analyze_critic(data, threshold=-0.5):\n",
    "    \"\"\"Calculate classification metrics per timestep\"\"\"\n",
    "    timesteps = np.unique(data['timesteps'])\n",
    "    metrics = {t: {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0} for t in timesteps}\n",
    "    \n",
    "    for t in timesteps:\n",
    "        mask = data['timesteps'] == t\n",
    "        values = data['values'][mask]\n",
    "        unsafe = data['unsafe'][mask]\n",
    "        \n",
    "        # Predictions (positive = unsafe prediction)\n",
    "        preds = values < threshold\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        metrics[t]['TP'] = np.sum(preds & unsafe)\n",
    "        metrics[t]['FP'] = np.sum(preds & ~unsafe)\n",
    "        metrics[t]['TN'] = np.sum(~preds & ~unsafe)\n",
    "        metrics[t]['FN'] = np.sum(~preds & unsafe)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics, scheduler):\n",
    "    \"\"\"Plot metrics over diffusion steps\"\"\"\n",
    "    timesteps = sorted(metrics.keys(), reverse=True)  # From noise to clean\n",
    "    steps = scheduler.timesteps.cpu().numpy()\n",
    "    \n",
    "    # Convert to rates\n",
    "    tp_rate = [metrics[t]['TP']/(metrics[t]['TP']+metrics[t]['FN']) if (metrics[t]['TP']+metrics[t]['FN']) > 0 else 0 for t in timesteps]\n",
    "    fp_rate = [metrics[t]['FP']/(metrics[t]['FP']+metrics[t]['TN']) if (metrics[t]['FP']+metrics[t]['TN']) > 0 else 0 for t in timesteps]\n",
    "    tn_rate = [metrics[t]['TN']/(metrics[t]['TN']+metrics[t]['FP']) if (metrics[t]['TN']+metrics[t]['FP']) > 0 else 0 for t in timesteps]\n",
    "    fn_rate = [metrics[t]['FN']/(metrics[t]['TP']+metrics[t]['FN']) if (metrics[t]['TP']+metrics[t]['FN']) > 0 else 0 for t in timesteps]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot 1: TP/FP Rates\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, tp_rate, label='TP Rate')\n",
    "    plt.plot(steps, fp_rate, label='FP Rate')\n",
    "    plt.title('True/False Positive Rates')\n",
    "    plt.xlabel('Diffusion Timestep')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.gca().invert_xaxis()  # Reverse to show denoising progression\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: TN/FN Rates\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, tn_rate, label='TN Rate')\n",
    "    plt.plot(steps, fn_rate, label='FN Rate')\n",
    "    plt.title('True/False Negative Rates')\n",
    "    plt.xlabel('Diffusion Timestep')\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "data = collect_critic_data(trainer, num_trajectories=10000)\n",
    "metrics = analyze_critic(data)\n",
    "plot_metrics(metrics, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EAIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
